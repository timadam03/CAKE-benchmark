{
  "metadata": {
    "name": "Full",
    "description": "CAKE (Full) \u2014 Cloud Architecture Knowledge Evaluation benchmark",
    "total_questions": 188,
    "distribution": {
      "by_skill": {
        "analyze": 60,
        "design": 50,
        "implement": 28,
        "recall": 50
      },
      "by_topic": {
        "architectural_patterns": 71,
        "cloud_deployment": 35,
        "decomposition": 33,
        "quality_attributes": 43,
        "technical_debt": 6
      },
      "by_format": {
        "free_response": 58,
        "mcq": 130
      }
    }
  },
  "questions": [
    {
      "id": "Q001",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What are the three states of a Circuit Breaker pattern?",
      "options": [
        "Open, Closed, Half-Open",
        "Active, Inactive, Pending",
        "Running, Stopped, Paused",
        "Connected, Disconnected, Reconnecting"
      ],
      "correct_answer": "Open, Closed, Half-Open",
      "explanation": "The Circuit Breaker pattern uses three states: Closed (normal operation), Open (blocking requests after failures exceed threshold), and Half-Open (testing if service has recovered).",
      "concept_ids": [
        "CONCEPT-001"
      ],
      "sources": [
        "microservices.io",
        "Azure Patterns"
      ]
    },
    {
      "id": "Q002",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What does CQRS stand for?",
      "options": [
        "Command Query Responsibility Segregation",
        "Concurrent Query Response System",
        "Central Query Routing Service",
        "Command Queue Read Separation"
      ],
      "correct_answer": "Command Query Responsibility Segregation",
      "explanation": "CQRS (Command Query Responsibility Segregation) separates read and write models, enabling independent optimization and scaling of each.",
      "concept_ids": [
        "CONCEPT-003"
      ],
      "sources": [
        "Martin Fowler",
        "microservices.io"
      ]
    },
    {
      "id": "Q003",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What does RTO stand for in disaster recovery?",
      "options": [
        "Recovery Time Objective",
        "Real-Time Operations",
        "Rollback Transaction Order",
        "Resource Timeout Option"
      ],
      "correct_answer": "Recovery Time Objective",
      "explanation": "RTO (Recovery Time Objective) is the maximum acceptable downtime after a disaster before business impact becomes unacceptable.",
      "concept_ids": [
        "CONCEPT-034"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q004",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "easy",
      "format": "mcq",
      "question": "Which pattern persists state as a sequence of immutable events rather than current state?",
      "options": [
        "Event Sourcing",
        "CQRS",
        "Saga",
        "Outbox"
      ],
      "correct_answer": "Event Sourcing",
      "explanation": "Event Sourcing persists business entities as a sequence of state-changing events, enabling complete audit trails, temporal queries, and event replay.",
      "concept_ids": [
        "CONCEPT-004"
      ],
      "sources": [
        "Martin Fowler",
        "microservices.io"
      ]
    },
    {
      "id": "Q005",
      "skill": "recall",
      "topic": "decomposition",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is Conway's Law about?",
      "options": [
        "Organizations design systems that mirror their communication structure",
        "Systems should be designed for maximum performance",
        "All microservices should be the same size",
        "Databases should be normalized to third normal form"
      ],
      "correct_answer": "Organizations design systems that mirror their communication structure",
      "explanation": "Conway's Law states that organizations design systems that mirror their communication structure, meaning team structure influences system architecture.",
      "concept_ids": [
        "CONCEPT-043"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q006",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What are the three pillars of observability?",
      "options": [
        "Logs, Metrics, Traces",
        "CPU, Memory, Disk",
        "Latency, Throughput, Errors",
        "Availability, Durability, Scalability"
      ],
      "correct_answer": "Logs, Metrics, Traces",
      "explanation": "The three pillars of observability are logs (event records), metrics (numerical measurements), and traces (request flow across services).",
      "concept_ids": [
        "CONCEPT-021"
      ],
      "sources": [
        "AWS Well-Architected",
        "microservices.io"
      ]
    },
    {
      "id": "Q007",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What pattern uses a queue as a buffer between producers and consumers to smooth load spikes?",
      "options": [
        "Queue-Based Load Leveling",
        "Circuit Breaker",
        "Bulkhead",
        "Throttling"
      ],
      "correct_answer": "Queue-Based Load Leveling",
      "explanation": "Queue-Based Load Leveling uses a queue to buffer requests, allowing consumers to process at their own pace regardless of producer load.",
      "concept_ids": [
        "CONCEPT-012"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q008",
      "skill": "recall",
      "topic": "decomposition",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What does the Strangler Fig pattern help with?",
      "options": [
        "Incrementally migrating legacy systems to new architecture",
        "Improving database performance",
        "Implementing authentication",
        "Scaling horizontally"
      ],
      "correct_answer": "Incrementally migrating legacy systems to new architecture",
      "explanation": "The Strangler Fig pattern enables incremental migration of legacy systems by building new functionality alongside the old system and gradually transferring traffic.",
      "concept_ids": [
        "CONCEPT-008"
      ],
      "sources": [
        "Martin Fowler",
        "Azure Patterns"
      ]
    },
    {
      "id": "Q009",
      "skill": "recall",
      "topic": "cloud_deployment",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is Infrastructure as Code (IaC)?",
      "options": [
        "Managing infrastructure through code and automation",
        "Writing application code for cloud",
        "Coding inside virtual machines",
        "Using code editors in the cloud"
      ],
      "correct_answer": "Managing infrastructure through code and automation",
      "explanation": "Infrastructure as Code (IaC) is the practice of defining and managing infrastructure through code, enabling version control, testing, and reproducibility.",
      "concept_ids": [
        "CONCEPT-035"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q010",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What security principle grants only the minimum permissions necessary for a task?",
      "options": [
        "Principle of Least Privilege",
        "Defense in Depth",
        "Zero Trust",
        "Separation of Duties"
      ],
      "correct_answer": "Principle of Least Privilege",
      "explanation": "The Principle of Least Privilege grants only the minimum permissions necessary to perform a task, reducing attack surface and potential damage.",
      "concept_ids": [
        "CONCEPT-030"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q011",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "In the Saga pattern, what are compensating transactions used for?",
      "options": [
        "Undoing the effects of previous transactions when rollback is needed",
        "Speeding up transaction processing",
        "Combining multiple transactions into one",
        "Logging transaction history"
      ],
      "correct_answer": "Undoing the effects of previous transactions when rollback is needed",
      "explanation": "Compensating transactions in a Saga undo the effects of previously completed local transactions when a later step fails and rollback is required.",
      "concept_ids": [
        "CONCEPT-002",
        "CONCEPT-080"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q012",
      "skill": "recall",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "In Domain-Driven Design, what are the three types of subdomains?",
      "options": [
        "Core, Supporting, Generic",
        "Primary, Secondary, Tertiary",
        "Main, Helper, Utility",
        "Business, Technical, Infrastructure"
      ],
      "correct_answer": "Core, Supporting, Generic",
      "explanation": "DDD classifies subdomains as Core (key differentiators), Supporting (related but not differentiating), and Generic (business-agnostic, suitable for off-the-shelf solutions).",
      "concept_ids": [
        "CONCEPT-017"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q013",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What does the Transactional Outbox pattern solve?",
      "options": [
        "Atomically updating database and publishing messages without 2PC",
        "Improving query performance",
        "Managing service discovery",
        "Implementing authentication"
      ],
      "correct_answer": "Atomically updating database and publishing messages without 2PC",
      "explanation": "The Transactional Outbox pattern writes messages to an outbox table in the same database transaction as business updates, then publishes them separately.",
      "concept_ids": [
        "CONCEPT-019"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q014",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What does RPO stand for in disaster recovery?",
      "options": [
        "Recovery Point Objective",
        "Recovery Process Order",
        "Resource Planning Optimization",
        "Redundancy Protocol Option"
      ],
      "correct_answer": "Recovery Point Objective",
      "explanation": "RPO (Recovery Point Objective) is the maximum acceptable amount of data loss measured in time - how far back in time you can afford to lose data.",
      "concept_ids": [
        "CONCEPT-034"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q015",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is the Bulkhead pattern named after?",
      "options": [
        "Ship compartments that prevent total flooding",
        "Building support structures",
        "Electronic circuit components",
        "Bridge construction elements"
      ],
      "correct_answer": "Ship compartments that prevent total flooding",
      "explanation": "The Bulkhead pattern is named after ship compartments that prevent total flooding if one section is compromised, similarly isolating failures in software.",
      "concept_ids": [
        "CONCEPT-009"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q016",
      "skill": "recall",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is a Service Mesh?",
      "options": [
        "Infrastructure layer handling service-to-service communication with sidecar proxies",
        "A network of microservices",
        "A type of container orchestration",
        "A load balancing algorithm"
      ],
      "correct_answer": "Infrastructure layer handling service-to-service communication with sidecar proxies",
      "explanation": "A Service Mesh is an infrastructure layer that handles service-to-service communication with features like mTLS, traffic management, and observability via sidecar proxies.",
      "concept_ids": [
        "CONCEPT-046"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q017",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is the difference between Sidecar and Ambassador patterns?",
      "options": [
        "Sidecar is co-located for general functionality; Ambassador specifically handles outbound connectivity",
        "They are identical patterns with different names",
        "Sidecar is for databases; Ambassador is for APIs",
        "Ambassador is older; Sidecar is the modern version"
      ],
      "correct_answer": "Sidecar is co-located for general functionality; Ambassador specifically handles outbound connectivity",
      "explanation": "Sidecar provides general supporting functionality co-located with the main app, while Ambassador specifically proxies outbound connectivity features like circuit breaking and routing.",
      "concept_ids": [
        "CONCEPT-013",
        "CONCEPT-014"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q018",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is Defense in Depth?",
      "options": [
        "Applying security controls at multiple layers",
        "Using the strongest possible encryption",
        "Having deep security expertise",
        "Defending against depth-first attacks"
      ],
      "correct_answer": "Applying security controls at multiple layers",
      "explanation": "Defense in Depth applies security controls at multiple layers (network, host, application, data) so that breach of one layer doesn't compromise the system.",
      "concept_ids": [
        "CONCEPT-031"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q019",
      "skill": "recall",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is a Bounded Context in Domain-Driven Design?",
      "options": [
        "A boundary where domain terms have specific, consistent meanings",
        "A limit on the size of a microservice",
        "A container for database transactions",
        "A security perimeter around services"
      ],
      "correct_answer": "A boundary where domain terms have specific, consistent meanings",
      "explanation": "A Bounded Context is a DDD pattern that divides large domain models into segments with explicit boundaries where terms have specific, consistent meanings.",
      "concept_ids": [
        "CONCEPT-018"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q020",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What does 'Smart Endpoints, Dumb Pipes' mean in microservices?",
      "options": [
        "Keep business logic in services while using simple communication infrastructure",
        "Use intelligent network equipment",
        "Make endpoints smarter over time",
        "Avoid using message queues"
      ],
      "correct_answer": "Keep business logic in services while using simple communication infrastructure",
      "explanation": "Smart Endpoints, Dumb Pipes means keeping business logic in services (smart endpoints) while using simple, lightweight communication infrastructure (dumb pipes).",
      "concept_ids": [
        "CONCEPT-064"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q021",
      "skill": "recall",
      "topic": "technical_debt",
      "difficulty": "easy",
      "format": "mcq",
      "question": "According to the Technical Debt metaphor, what represents 'interest payments'?",
      "options": [
        "Extra effort required to add new features due to poor code quality",
        "Actual financial costs of development",
        "Time spent in meetings",
        "Server maintenance costs"
      ],
      "correct_answer": "Extra effort required to add new features due to poor code quality",
      "explanation": "In the Technical Debt metaphor, interest payments are the extra effort required to add features or fix bugs due to suboptimal code quality.",
      "concept_ids": [
        "CONCEPT-029"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q022",
      "skill": "recall",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is the 'Microservice Premium'?",
      "options": [
        "The overhead cost of managing multiple services compared to a monolith",
        "A pricing model for microservice platforms",
        "Premium features in microservice frameworks",
        "Extra performance from microservices"
      ],
      "correct_answer": "The overhead cost of managing multiple services compared to a monolith",
      "explanation": "The Microservice Premium is the additional overhead (complexity, operational cost) of managing multiple services compared to a monolith.",
      "concept_ids": [
        "CONCEPT-060"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q023",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is an Idempotent Consumer?",
      "options": [
        "A consumer that produces the same result when processing the same message multiple times",
        "A consumer that only processes unique messages",
        "A consumer that consumes messages in order",
        "A consumer that never fails"
      ],
      "correct_answer": "A consumer that produces the same result when processing the same message multiple times",
      "explanation": "An Idempotent Consumer can safely process the same message multiple times while producing the same result, handling message redelivery gracefully.",
      "concept_ids": [
        "CONCEPT-024"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q024",
      "skill": "recall",
      "topic": "cloud_deployment",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is the main benefit of serverless deployment?",
      "options": [
        "No infrastructure management required",
        "Guaranteed lowest cost",
        "Fastest execution speed",
        "Unlimited execution time"
      ],
      "correct_answer": "No infrastructure management required",
      "explanation": "Serverless deployment eliminates infrastructure management, allowing developers to focus on code while the platform handles scaling and billing per-use.",
      "concept_ids": [
        "CONCEPT-026"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q025",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is a Health Check API used for?",
      "options": [
        "Reporting service's ability to handle requests for routing and self-healing decisions",
        "Checking developer health",
        "Monitoring server hardware",
        "Validating API schemas"
      ],
      "correct_answer": "Reporting service's ability to handle requests for routing and self-healing decisions",
      "explanation": "A Health Check API returns whether a service can handle requests, used by platforms for routing decisions and triggering self-healing actions.",
      "concept_ids": [
        "CONCEPT-022"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q026",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "What is the key difference between Choreography and Orchestration in saga coordination?",
      "options": [
        "Choreography uses events for decentralized control; Orchestration uses a central coordinator",
        "Choreography is faster; Orchestration is more reliable",
        "Choreography is for small sagas; Orchestration is for large ones",
        "They are identical approaches with different names"
      ],
      "correct_answer": "Choreography uses events for decentralized control; Orchestration uses a central coordinator",
      "explanation": "Choreography coordinates sagas through events where services react independently. Orchestration uses a central coordinator that directs participants.",
      "concept_ids": [
        "CONCEPT-079"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q027",
      "skill": "recall",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is GitOps?",
      "options": [
        "Using Git as single source of truth for infrastructure with automated synchronization",
        "Using Git for code versioning",
        "Operating systems for Git servers",
        "Git operations in production"
      ],
      "correct_answer": "Using Git as single source of truth for infrastructure with automated synchronization",
      "explanation": "GitOps uses Git as the single source of truth for declarative infrastructure and applications, with automated synchronization to live environments.",
      "concept_ids": [
        "CONCEPT-036"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q028",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "According to CAP theorem, what three properties cannot all be guaranteed simultaneously?",
      "options": [
        "Consistency, Availability, Partition tolerance",
        "Capacity, Agility, Performance",
        "Cost, Accuracy, Precision",
        "Compliance, Auditing, Privacy"
      ],
      "correct_answer": "Consistency, Availability, Partition tolerance",
      "explanation": "CAP theorem states that a distributed system can only guarantee two of three properties: Consistency, Availability, and Partition tolerance.",
      "concept_ids": [
        "CONCEPT-041"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q029",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is Chaos Engineering?",
      "options": [
        "Deliberately injecting failures to test system resilience",
        "Managing chaotic development processes",
        "Debugging complex systems",
        "Random code refactoring"
      ],
      "correct_answer": "Deliberately injecting failures to test system resilience",
      "explanation": "Chaos Engineering deliberately injects failures into systems to test resilience and discover weaknesses before they cause production incidents.",
      "concept_ids": [
        "CONCEPT-037"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q030",
      "skill": "recall",
      "topic": "decomposition",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is the 'Database per Service' pattern?",
      "options": [
        "Each microservice owns and manages its private database",
        "One database shared across all services",
        "Multiple databases per service",
        "No databases in microservices"
      ],
      "correct_answer": "Each microservice owns and manages its private database",
      "explanation": "Database per Service means each microservice owns its private database, accessible only through its API, enabling loose coupling and polyglot persistence.",
      "concept_ids": [
        "CONCEPT-005"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q031",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is Polyglot Persistence?",
      "options": [
        "Using different database technologies for different services based on their needs",
        "Storing data in multiple languages",
        "Using a single database that supports multiple query languages",
        "Persisting code in multiple programming languages"
      ],
      "correct_answer": "Using different database technologies for different services based on their needs",
      "explanation": "Polyglot Persistence uses different database technologies optimized for different services' needs rather than a single shared database for all.",
      "concept_ids": [
        "CONCEPT-045"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q032",
      "skill": "recall",
      "topic": "cloud_deployment",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is Blue-Green Deployment?",
      "options": [
        "Maintaining two production environments and switching traffic between them",
        "Using blue and green colored servers",
        "Deploying to staging then production",
        "A type of container deployment"
      ],
      "correct_answer": "Maintaining two production environments and switching traffic between them",
      "explanation": "Blue-Green Deployment maintains two production environments (blue and green), deploys to the inactive one, then switches traffic for instant rollback capability.",
      "concept_ids": [
        "CONCEPT-054"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q033",
      "skill": "recall",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is Canary Deployment?",
      "options": [
        "Gradually rolling out changes to a subset of users before full deployment",
        "Deploying to test environments first",
        "Using canary servers for testing",
        "A type of blue-green deployment"
      ],
      "correct_answer": "Gradually rolling out changes to a subset of users before full deployment",
      "explanation": "Canary Deployment gradually rolls out changes to a small subset of users, detecting issues with limited blast radius before full deployment.",
      "concept_ids": [
        "CONCEPT-055"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q034",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is an API Gateway?",
      "options": [
        "A single entry point that routes requests to appropriate backend services",
        "A database for API definitions",
        "A tool for generating API documentation",
        "A testing framework for APIs"
      ],
      "correct_answer": "A single entry point that routes requests to appropriate backend services",
      "explanation": "An API Gateway is a single entry point that routes requests to backend services, providing protocol translation, authentication, and response aggregation.",
      "concept_ids": [
        "CONCEPT-006"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q035",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is the Anti-Corruption Layer pattern used for?",
      "options": [
        "Translating between systems with different semantics to prevent legacy models from corrupting new designs",
        "Preventing data corruption in databases",
        "Encrypting data in transit",
        "Validating input data"
      ],
      "correct_answer": "Translating between systems with different semantics to prevent legacy models from corrupting new designs",
      "explanation": "The Anti-Corruption Layer translates between systems with different semantics, preventing legacy or external system models from corrupting new application design.",
      "concept_ids": [
        "CONCEPT-015"
      ],
      "sources": [
        "Azure Patterns",
        "microservices.io"
      ]
    },
    {
      "id": "Q036",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is Horizontal Scaling?",
      "options": [
        "Adding more instances to handle increased load",
        "Increasing capacity of existing instances",
        "Scaling database schemas",
        "Widening network bandwidth"
      ],
      "correct_answer": "Adding more instances to handle increased load",
      "explanation": "Horizontal Scaling (scale out) adds more instances to handle load, unlike Vertical Scaling which increases capacity of existing instances.",
      "concept_ids": [
        "CONCEPT-032"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q037",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "What problem does Two-Phase Commit (2PC) solve, and why is it problematic for microservices?",
      "options": [
        "Ensures distributed transaction atomicity but blocks and reduces availability",
        "Improves query performance but increases complexity",
        "Enables service discovery but requires central coordinator",
        "Provides encryption but slows communication"
      ],
      "correct_answer": "Ensures distributed transaction atomicity but blocks and reduces availability",
      "explanation": "2PC ensures all participants in a distributed transaction commit or abort together, but it's blocking and reduces availability, making it problematic for microservices.",
      "concept_ids": [
        "CONCEPT-044"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q038",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is Eventual Consistency?",
      "options": [
        "Guarantee that reads will eventually return the last updated value if no new updates are made",
        "Immediate consistency across all replicas",
        "Consistency that eventually fails",
        "A weaker form of ACID consistency"
      ],
      "correct_answer": "Guarantee that reads will eventually return the last updated value if no new updates are made",
      "explanation": "Eventual Consistency guarantees that if no new updates are made, eventually all reads will return the last updated value, trading immediate consistency for availability.",
      "concept_ids": [
        "CONCEPT-042"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q039",
      "skill": "recall",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is Service Discovery?",
      "options": [
        "Mechanism for services to find network locations of other services",
        "Finding new microservices to implement",
        "Discovering service documentation",
        "A testing technique"
      ],
      "correct_answer": "Mechanism for services to find network locations of other services",
      "explanation": "Service Discovery is a mechanism for services to find network locations of other services, using client-side, server-side, or DNS-based approaches.",
      "concept_ids": [
        "CONCEPT-077"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q040",
      "skill": "recall",
      "topic": "technical_debt",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What does Semantic Versioning's MAJOR version number indicate?",
      "options": [
        "Breaking changes that may require modifications to dependent code",
        "New features with backward compatibility",
        "Bug fixes only",
        "Documentation updates"
      ],
      "correct_answer": "Breaking changes that may require modifications to dependent code",
      "explanation": "In Semantic Versioning (MAJOR.MINOR.PATCH), incrementing MAJOR indicates breaking changes that may require modifications to dependent code.",
      "concept_ids": [
        "CONCEPT-082"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q041",
      "skill": "recall",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is the 'Shared Database' anti-pattern?",
      "options": [
        "Multiple services sharing a single database, creating tight coupling",
        "Using a database that supports sharing",
        "Sharing database credentials",
        "Having backup databases"
      ],
      "correct_answer": "Multiple services sharing a single database, creating tight coupling",
      "explanation": "The Shared Database anti-pattern occurs when multiple services share a single database, creating tight coupling and preventing independent evolution.",
      "concept_ids": [
        "CONCEPT-061"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q042",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is the Backend for Frontend (BFF) pattern?",
      "options": [
        "Separate API gateways optimized for each client type (web, mobile, etc.)",
        "Frontend code that runs on backend servers",
        "Backend services for frontend developers",
        "A frontend testing framework"
      ],
      "correct_answer": "Separate API gateways optimized for each client type (web, mobile, etc.)",
      "explanation": "Backend for Frontend (BFF) creates separate API gateways for each client type (web, mobile, third-party), each providing an optimized API.",
      "concept_ids": [
        "CONCEPT-007"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q043",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is Distributed Tracing?",
      "options": [
        "Tracking requests across services using unique correlation IDs",
        "Distributing trace files across servers",
        "Tracing network routes",
        "Debugging distributed systems offline"
      ],
      "correct_answer": "Tracking requests across services using unique correlation IDs",
      "explanation": "Distributed Tracing assigns unique IDs to requests, passes them through all services, and records details for end-to-end visibility and debugging.",
      "concept_ids": [
        "CONCEPT-020"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q044",
      "skill": "recall",
      "topic": "cloud_deployment",
      "difficulty": "easy",
      "format": "mcq",
      "question": "What is Auto Scaling?",
      "options": [
        "Automatically adjusting compute capacity based on demand",
        "Scaling images automatically",
        "Automatic schema scaling",
        "Self-scaling containers"
      ],
      "correct_answer": "Automatically adjusting compute capacity based on demand",
      "explanation": "Auto Scaling automatically adjusts compute capacity based on demand metrics to maintain performance while optimizing costs.",
      "concept_ids": [
        "CONCEPT-040"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q045",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "What is the Valet Key pattern?",
      "options": [
        "Issuing time-limited tokens for direct resource access without going through the application",
        "A pattern for key management",
        "Storing keys in valet services",
        "Using service accounts"
      ],
      "correct_answer": "Issuing time-limited tokens for direct resource access without going through the application",
      "explanation": "The Valet Key pattern issues time-limited tokens providing direct access to specific resources, allowing clients to bypass the application for storage operations.",
      "concept_ids": [
        "CONCEPT-050"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q046",
      "skill": "recall",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is a Blameless Post-Mortem?",
      "options": [
        "Incident review focused on systemic improvements rather than individual blame",
        "Post-mortem with no conclusions",
        "Review without documentation",
        "Anonymous incident reviews"
      ],
      "correct_answer": "Incident review focused on systemic improvements rather than individual blame",
      "explanation": "Blameless Post-Mortems focus on systemic improvements rather than individual blame, encouraging honest reporting and organizational learning.",
      "concept_ids": [
        "CONCEPT-070"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q047",
      "skill": "recall",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "mcq",
      "question": "What is Kubernetes Server-Side Apply?",
      "options": [
        "Moving apply logic to API server with field ownership tracking",
        "Applying configurations from server",
        "Server-side rendering for Kubernetes",
        "Applying patches server-side"
      ],
      "correct_answer": "Moving apply logic to API server with field ownership tracking",
      "explanation": "Server-Side Apply moves apply logic to the API server, tracks field ownership per manager, and detects conflicts when multiple managers modify same fields.",
      "concept_ids": [
        "CONCEPT-072"
      ],
      "sources": [
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q048",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is the Competing Consumers pattern?",
      "options": [
        "Multiple consumers pulling from the same queue with each message going to one consumer",
        "Consumers competing for resources",
        "Consumer load balancing",
        "Priority-based consumption"
      ],
      "correct_answer": "Multiple consumers pulling from the same queue with each message going to one consumer",
      "explanation": "Competing Consumers has multiple consumer instances pulling from the same queue, with the messaging system ensuring each message goes to only one consumer.",
      "concept_ids": [
        "CONCEPT-051"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q049",
      "skill": "recall",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "mcq",
      "question": "What is Martin Fowler's 'MonolithFirst' recommendation?",
      "options": [
        "Start with a monolith to discover service boundaries before adopting microservices",
        "Always use monoliths",
        "Build microservices first, then merge into monolith",
        "Monoliths are faster than microservices"
      ],
      "correct_answer": "Start with a monolith to discover service boundaries before adopting microservices",
      "explanation": "MonolithFirst recommends starting with a monolith even for complex applications to discover proper service boundaries before committing to distributed architecture.",
      "concept_ids": [
        "CONCEPT-027"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q050",
      "skill": "recall",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "What is API Composition?",
      "options": [
        "Implementing queries by calling services and performing in-memory joins",
        "Composing APIs from multiple definitions",
        "Creating composite API endpoints",
        "API documentation composition"
      ],
      "correct_answer": "Implementing queries by calling services and performing in-memory joins",
      "explanation": "API Composition implements queries spanning multiple services by calling each service and performing in-memory joins, simple but with latency trade-offs.",
      "concept_ids": [
        "CONCEPT-063"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q051",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team is experiencing cascading failures when one downstream service becomes slow. Requests pile up, exhausting thread pools. Which pattern combination would best address this?",
      "options": [
        "Circuit Breaker with Timeout and Bulkhead",
        "Retry with increased timeout",
        "API Gateway with caching",
        "Database per Service with sharding"
      ],
      "correct_answer": "Circuit Breaker with Timeout and Bulkhead",
      "explanation": "Circuit Breaker stops calling failing services, Timeout prevents indefinite waiting, and Bulkhead isolates resources so one failing dependency doesn't exhaust all threads.",
      "concept_ids": [
        "CONCEPT-001",
        "CONCEPT-009"
      ],
      "sources": [
        "Azure Patterns",
        "microservices.io"
      ]
    },
    {
      "id": "Q052",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A startup is building a new product with uncertain requirements. The CTO is debating between microservices and a monolith. What is the strongest argument for starting with a monolith?",
      "options": [
        "Service boundaries are hard to define correctly upfront, and refactoring across services is much harder than within a monolith",
        "Monoliths are always faster",
        "Microservices require more servers",
        "Monoliths don't need testing"
      ],
      "correct_answer": "Service boundaries are hard to define correctly upfront, and refactoring across services is much harder than within a monolith",
      "explanation": "Per MonolithFirst, defining correct service boundaries is extremely difficult initially, and refactoring functionality between services is much harder than within a monolith.",
      "concept_ids": [
        "CONCEPT-027",
        "CONCEPT-060"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q053",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "When should you choose Saga with Choreography over Orchestration?",
      "options": [
        "When you want loose coupling and services to evolve independently",
        "When you need a clear visualization of the business process",
        "When compensating transactions are complex",
        "When you have a small number of steps"
      ],
      "correct_answer": "When you want loose coupling and services to evolve independently",
      "explanation": "Choreography provides looser coupling as services react to events independently. Orchestration is better when you need clear process visualization or have complex compensation logic.",
      "concept_ids": [
        "CONCEPT-002",
        "CONCEPT-079"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q054",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A system requires both high availability and strong consistency for financial transactions. According to CAP theorem, what must be true?",
      "options": [
        "The system must handle network partitions by temporarily sacrificing either availability or consistency",
        "Both can be achieved simultaneously with enough resources",
        "CAP theorem doesn't apply to financial systems",
        "Strong consistency is impossible in distributed systems"
      ],
      "correct_answer": "The system must handle network partitions by temporarily sacrificing either availability or consistency",
      "explanation": "CAP theorem states you can't have all three. During partitions, you must choose: sacrifice availability (reject requests) or consistency (allow stale reads).",
      "concept_ids": [
        "CONCEPT-041"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q055",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A large e-commerce company has 50 microservices. They're experiencing issues where the same concept (e.g., 'Customer') has different meanings across teams. What DDD concept should they apply?",
      "options": [
        "Bounded Contexts with explicit context mapping between teams",
        "Shared database for common entities",
        "Single unified data model",
        "Microservice consolidation"
      ],
      "correct_answer": "Bounded Contexts with explicit context mapping between teams",
      "explanation": "Bounded Contexts divide the domain into segments where terms have consistent meanings. Context mapping explicitly defines relationships between contexts.",
      "concept_ids": [
        "CONCEPT-018"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q056",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A service needs to update its database and publish an event atomically without using distributed transactions. Assuming the message broker doesn't support transactions with the database, what pattern is most appropriate?",
      "options": [
        "Transactional Outbox pattern",
        "Two-Phase Commit",
        "Saga pattern",
        "CQRS pattern"
      ],
      "correct_answer": "Transactional Outbox pattern",
      "explanation": "Transactional Outbox writes events to an outbox table in the same database transaction as business data, then a separate process publishes them to the broker.",
      "concept_ids": [
        "CONCEPT-019"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q057",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team is considering serverless (Lambda) vs containers (EKS) for a new service. The service handles sporadic traffic with long idle periods but has strict cold start requirements. What's the best approach?",
      "options": [
        "Containers, as serverless cold starts may violate latency requirements during idle periods",
        "Serverless, as it's always cheaper",
        "Both are equivalent for this use case",
        "Neither; use VMs instead"
      ],
      "correct_answer": "Containers, as serverless cold starts may violate latency requirements during idle periods",
      "explanation": "Serverless cold starts occur after idle periods. If strict latency is required and traffic is sporadic, containers with minimum instances may be more appropriate.",
      "concept_ids": [
        "CONCEPT-026"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q058",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A company's RTO is 4 hours and RPO is 1 hour. They currently have daily backups. What's wrong with their disaster recovery strategy?",
      "options": [
        "Daily backups can result in up to 24 hours of data loss, exceeding their 1-hour RPO",
        "4-hour RTO is too aggressive",
        "Daily backups are sufficient for any RTO",
        "RPO and RTO should be equal"
      ],
      "correct_answer": "Daily backups can result in up to 24 hours of data loss, exceeding their 1-hour RPO",
      "explanation": "With daily backups, the worst case data loss is 24 hours (just before the next backup), far exceeding the 1-hour RPO requirement.",
      "concept_ids": [
        "CONCEPT-033",
        "CONCEPT-034"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q059",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "An application has a 10:1 read-to-write ratio with complex queries spanning multiple aggregates. Writes require strong consistency. What pattern best addresses this?",
      "options": [
        "CQRS with separate read and write models",
        "Single normalized database",
        "Event Sourcing only",
        "Database sharding"
      ],
      "correct_answer": "CQRS with separate read and write models",
      "explanation": "CQRS allows a strongly consistent write model while maintaining optimized, denormalized read models for complex queries. The high read ratio justifies the complexity.",
      "concept_ids": [
        "CONCEPT-003"
      ],
      "sources": [
        "Martin Fowler",
        "microservices.io"
      ]
    },
    {
      "id": "Q060",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A legacy monolith has 500K lines of code and 200 database tables. The team wants to extract microservices. What's the most significant risk of using Strangler Fig pattern?",
      "options": [
        "Managing data consistency between the legacy system and new services during the transition",
        "The pattern is too complex to implement",
        "It requires rewriting all code at once",
        "Legacy systems cannot coexist with microservices"
      ],
      "correct_answer": "Managing data consistency between the legacy system and new services during the transition",
      "explanation": "During Strangler Fig migration, both systems may need to access or modify the same data, creating consistency challenges that require careful synchronization strategies.",
      "concept_ids": [
        "CONCEPT-008"
      ],
      "sources": [
        "Martin Fowler",
        "Azure Patterns"
      ]
    },
    {
      "id": "Q061",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "When would Event Sourcing be a poor choice despite its benefits?",
      "options": [
        "For simple CRUD applications with no audit requirements and a team unfamiliar with event-driven patterns",
        "When you need a complete audit trail",
        "When debugging production issues is important",
        "When you want temporal queries"
      ],
      "correct_answer": "For simple CRUD applications with no audit requirements and a team unfamiliar with event-driven patterns",
      "explanation": "Event Sourcing adds significant complexity. For simple CRUD without audit needs, and when teams lack experience, traditional state-based persistence is more appropriate.",
      "concept_ids": [
        "CONCEPT-004"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q062",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team implements aggressive retry logic (10 retries, no backoff) when their downstream payment service is slow. Traffic to the payment service increases 10x. What happened?",
      "options": [
        "The retry storm amplified load on the already struggling service, potentially causing complete failure",
        "The payment service got faster from the practice",
        "Retries always help reliability",
        "The retries reduced overall latency"
      ],
      "correct_answer": "The retry storm amplified load on the already struggling service, potentially causing complete failure",
      "explanation": "Aggressive retries without backoff create a 'retry storm' that amplifies load on struggling services, potentially causing complete failure instead of recovery.",
      "concept_ids": [
        "CONCEPT-010",
        "CONCEPT-001"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q063",
      "skill": "analyze",
      "topic": "technical_debt",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team has accumulated technical debt in a rarely-modified module. A new feature requires extensive changes to a frequently-modified module with similar debt. Where should they prioritize debt reduction?",
      "options": [
        "The frequently-modified module, because interest payments are higher where code changes often",
        "The rarely-modified module, to prevent future problems",
        "Both equally",
        "Neither; ship the feature first"
      ],
      "correct_answer": "The frequently-modified module, because interest payments are higher where code changes often",
      "explanation": "Technical debt 'interest' is paid when modifying code. Debt in frequently-changed code has higher ongoing costs, so prioritize paying it down there.",
      "concept_ids": [
        "CONCEPT-029"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q064",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A company runs predictable workloads 24/7 and variable batch jobs nightly. They're currently using all on-demand EC2 instances. How should they optimize costs?",
      "options": [
        "Reserved instances for 24/7 workloads, Spot instances for fault-tolerant batch jobs",
        "All reserved instances",
        "All spot instances",
        "All on-demand instances but larger sizes"
      ],
      "correct_answer": "Reserved instances for 24/7 workloads, Spot instances for fault-tolerant batch jobs",
      "explanation": "Reserved instances provide significant discounts for predictable workloads. Spot instances offer even larger savings for interruptible, fault-tolerant batch processing.",
      "concept_ids": [
        "CONCEPT-038",
        "CONCEPT-039"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q065",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A mobile app makes 5 API calls to different microservices on each screen load. Users on cellular networks report slow performance. What pattern would most directly address this?",
      "options": [
        "Gateway Aggregation to combine multiple calls into a single request",
        "Circuit Breaker on each call",
        "Caching on the client",
        "Database optimization"
      ],
      "correct_answer": "Gateway Aggregation to combine multiple calls into a single request",
      "explanation": "Gateway Aggregation combines multiple backend calls into one client request, reducing latency on high-latency networks where round-trip time dominates.",
      "concept_ids": [
        "CONCEPT-049",
        "CONCEPT-006"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q066",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "mcq",
      "question": "Two teams are building services that both need customer data. Team A wants to call Team B's Customer Service API. Team B suggests sharing the database for performance. What's wrong with sharing the database?",
      "options": [
        "It creates tight coupling, preventing independent schema changes and deployments",
        "Databases can't be shared technically",
        "It would be too fast",
        "API calls are always better"
      ],
      "correct_answer": "It creates tight coupling, preventing independent schema changes and deployments",
      "explanation": "Shared databases create tight coupling between services, preventing independent evolution, schema changes, and deployments - violating the Database per Service principle.",
      "concept_ids": [
        "CONCEPT-005",
        "CONCEPT-061"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q067",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A security team mandates encryption at rest and in transit, least privilege access, and WAF. Despite implementing all, they suffer a breach via SQL injection. What principle was missing?",
      "options": [
        "Defense in Depth at the application layer - input validation and parameterized queries",
        "They needed stronger encryption",
        "Least privilege wasn't strict enough",
        "WAF should have caught everything"
      ],
      "correct_answer": "Defense in Depth at the application layer - input validation and parameterized queries",
      "explanation": "Defense in Depth requires controls at ALL layers including application code. SQL injection bypasses network controls; application-level defenses like parameterized queries are essential.",
      "concept_ids": [
        "CONCEPT-031"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q068",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A team using Event Sourcing finds that replaying millions of events to rebuild state takes 30 minutes. What technique should they use?",
      "options": [
        "Snapshots - periodically saving current state to reduce replay requirements",
        "Delete old events",
        "Use a traditional database instead",
        "Add more servers"
      ],
      "correct_answer": "Snapshots - periodically saving current state to reduce replay requirements",
      "explanation": "Snapshots periodically save the current state, so replay only needs to process events after the last snapshot, dramatically reducing rebuild time.",
      "concept_ids": [
        "CONCEPT-004"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q069",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team wants to add mTLS, observability, and traffic management to 20 microservices without modifying application code. What should they implement?",
      "options": [
        "Service Mesh with sidecar proxies",
        "API Gateway",
        "Application-level libraries in each service",
        "Network policies"
      ],
      "correct_answer": "Service Mesh with sidecar proxies",
      "explanation": "A Service Mesh handles these concerns via sidecar proxies without application code changes, providing uniform security and observability across all services.",
      "concept_ids": [
        "CONCEPT-046"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q070",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "An e-commerce site shows product listings with 5-minute stale data, but order placement requires real-time inventory. What consistency model supports this?",
      "options": [
        "Eventual consistency for reads with strong consistency for writes affecting inventory",
        "Strong consistency everywhere",
        "Eventual consistency everywhere",
        "No consistency guarantees"
      ],
      "correct_answer": "Eventual consistency for reads with strong consistency for writes affecting inventory",
      "explanation": "Product listings can tolerate eventual consistency (stale reads acceptable), but inventory updates during orders need strong consistency to prevent overselling.",
      "concept_ids": [
        "CONCEPT-042"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q071",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A Saga has 5 steps. Step 4 fails after steps 1-3 completed successfully. Step 2 sent an email notification. What challenge does this highlight?",
      "options": [
        "Some actions (like sending emails) cannot be truly compensated, only mitigated",
        "Sagas shouldn't have 5 steps",
        "Email should never be in a Saga",
        "Step 4 should never fail"
      ],
      "correct_answer": "Some actions (like sending emails) cannot be truly compensated, only mitigated",
      "explanation": "Not all actions are reversible. Sent emails can't be unsent - you can only send a correction email. This is a key challenge when designing Saga compensating transactions.",
      "concept_ids": [
        "CONCEPT-002",
        "CONCEPT-080"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q072",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A company with web, iOS, and Android apps is considering BFF vs single API Gateway. Their mobile apps need different data shapes and smaller payloads than web. What's the best approach?",
      "options": [
        "BFF pattern with separate gateways for web and mobile",
        "Single API Gateway serving all clients",
        "Direct service calls from clients",
        "GraphQL for all clients"
      ],
      "correct_answer": "BFF pattern with separate gateways for web and mobile",
      "explanation": "When different clients have significantly different requirements (data shapes, payload sizes), BFF allows optimizing each gateway for its specific client type.",
      "concept_ids": [
        "CONCEPT-007"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q073",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team sets Circuit Breaker thresholds to trip after 3 failures in 10 seconds. In production, the breaker trips during normal operation due to occasional network blips. What should they adjust?",
      "options": [
        "Increase the failure threshold or time window to account for transient failures",
        "Remove the circuit breaker",
        "Decrease the threshold for faster protection",
        "Increase timeout values"
      ],
      "correct_answer": "Increase the failure threshold or time window to account for transient failures",
      "explanation": "If the breaker trips during normal operation, the thresholds are too sensitive. Adjust to tolerate transient failures while still protecting against sustained outages.",
      "concept_ids": [
        "CONCEPT-001"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q074",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A Kubernetes cluster experiences a 'noisy neighbor' problem where one misbehaving controller overwhelms the API server. What KEP-implemented feature addresses this?",
      "options": [
        "API Priority and Fairness with priority levels and flow schemas",
        "Pod resource limits",
        "Network policies",
        "Horizontal Pod Autoscaling"
      ],
      "correct_answer": "API Priority and Fairness with priority levels and flow schemas",
      "explanation": "API Priority and Fairness classifies requests into priority levels with guaranteed capacity shares, protecting system-critical requests from being starved by misbehaving clients.",
      "concept_ids": [
        "CONCEPT-074"
      ],
      "sources": [
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q075",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A service uses message queues with competing consumers for order processing. Orders occasionally process twice. What's missing?",
      "options": [
        "Idempotent consumer implementation to handle duplicate message delivery",
        "More consumers",
        "Larger queue",
        "Synchronous processing"
      ],
      "correct_answer": "Idempotent consumer implementation to handle duplicate message delivery",
      "explanation": "With at-least-once delivery (common in message queues), consumers must be idempotent to handle redelivery safely and produce correct results on duplicate processing.",
      "concept_ids": [
        "CONCEPT-024",
        "CONCEPT-051"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q076",
      "skill": "analyze",
      "topic": "technical_debt",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team is planning a complete rewrite of their system 'because the code is too messy.' What consideration from Sacrificial Architecture suggests this might be appropriate?",
      "options": [
        "The system was designed for 10x current scale, and they're approaching 100x",
        "The code has some bugs",
        "The team doesn't like the programming language",
        "A new framework is available"
      ],
      "correct_answer": "The system was designed for 10x current scale, and they're approaching 100x",
      "explanation": "Sacrificial Architecture acknowledges systems designed for one scale may not suit another. Google's principle: design for 10X growth, plan to rewrite before 100X.",
      "concept_ids": [
        "CONCEPT-028"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q077",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A team runs monthly chaos engineering experiments but never finds issues. Their production system had 3 major outages last quarter. What's likely wrong?",
      "options": [
        "Their chaos experiments don't reflect real-world failure modes that caused the outages",
        "Chaos engineering doesn't work",
        "They should stop chaos engineering",
        "Monthly is too frequent"
      ],
      "correct_answer": "Their chaos experiments don't reflect real-world failure modes that caused the outages",
      "explanation": "Effective chaos engineering should recreate conditions that caused past failures. If experiments don't find issues but production fails, the experiments aren't realistic enough.",
      "concept_ids": [
        "CONCEPT-037"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q078",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "An organization integrated with a legacy ERP system whose data model pollutes their domain model. New code increasingly mirrors legacy concepts. What pattern should they have applied?",
      "options": [
        "Anti-Corruption Layer to translate between legacy and new domain models",
        "Shared database",
        "Direct integration",
        "Complete replacement"
      ],
      "correct_answer": "Anti-Corruption Layer to translate between legacy and new domain models",
      "explanation": "An Anti-Corruption Layer translates between systems with different semantics, preventing legacy models from corrupting the design of new applications.",
      "concept_ids": [
        "CONCEPT-015"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q079",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team uses feature flags extensively. After 2 years, they have 500 flags, many abandoned. What technical debt has accumulated?",
      "options": [
        "Dead code paths protected by abandoned flags, increasing complexity and maintenance burden",
        "Too many features",
        "Performance degradation from flag checks",
        "Security vulnerabilities"
      ],
      "correct_answer": "Dead code paths protected by abandoned flags, increasing complexity and maintenance burden",
      "explanation": "Feature flags that aren't cleaned up create dead code paths, increase testing complexity, and make the codebase harder to understand and maintain.",
      "concept_ids": [
        "CONCEPT-056",
        "CONCEPT-029"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q080",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A company's microservices team structure mirrors their organizational structure per Conway's Law. They want to change the architecture but can't restructure teams. What's a likely outcome?",
      "options": [
        "The new architecture will drift back toward the team structure over time",
        "Architecture will change teams automatically",
        "Conway's Law doesn't apply to microservices",
        "Teams don't affect architecture"
      ],
      "correct_answer": "The new architecture will drift back toward the team structure over time",
      "explanation": "Conway's Law suggests systems reflect organizational communication structures. Without changing team structure, the architecture will naturally drift to match teams.",
      "concept_ids": [
        "CONCEPT-043"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q081",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A public API has 3 active versions (v1, v2, v3). v1 has 1000 users, v2 has 100, v3 is latest. Which version should be prioritized for deprecation?",
      "options": [
        "v2, as it has fewer users and is between stable versions",
        "v1, as it's oldest",
        "v3, as it's newest",
        "Deprecate all at once"
      ],
      "correct_answer": "v2, as it has fewer users and is between stable versions",
      "explanation": "v2 with fewest users minimizes migration impact. Users likely should migrate to v3. v1's larger user base makes it harder to deprecate first.",
      "concept_ids": [
        "CONCEPT-057",
        "CONCEPT-068"
      ],
      "sources": [
        "ADRs",
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q082",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A distributed system has logs in each service but debugging cross-service issues takes hours. What's the most impactful improvement?",
      "options": [
        "Implement distributed tracing with correlation IDs across all services",
        "Add more logging",
        "Centralize logs without correlation",
        "Debug each service separately"
      ],
      "correct_answer": "Implement distributed tracing with correlation IDs across all services",
      "explanation": "Distributed tracing with correlation IDs enables following a single request across all services, dramatically reducing time to debug cross-service issues.",
      "concept_ids": [
        "CONCEPT-020",
        "CONCEPT-021"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q083",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A service sends events to Kafka. Sometimes events arrive out of order at consumers. Order Service events must be processed in order per order ID. What's the solution?",
      "options": [
        "Use order ID as the partition key to ensure events for the same order go to the same partition",
        "Use a single partition",
        "Add timestamps and sort",
        "Switch to synchronous calls"
      ],
      "correct_answer": "Use order ID as the partition key to ensure events for the same order go to the same partition",
      "explanation": "Kafka guarantees order within a partition. Using order ID as the key ensures all events for an order go to the same partition and are processed in order.",
      "concept_ids": [
        "CONCEPT-023"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q084",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team deploys 10 times per day using GitOps. They accidentally deploy a broken config. The rollback process takes 30 minutes. What should they improve?",
      "options": [
        "Automate rollback to previous Git commit, which should be instant with GitOps",
        "Deploy less frequently",
        "Add more approvals",
        "Stop using GitOps"
      ],
      "correct_answer": "Automate rollback to previous Git commit, which should be instant with GitOps",
      "explanation": "With GitOps, rollback should be as simple as reverting to a previous Git commit. If it takes 30 minutes, they're not leveraging GitOps' declarative rollback capability.",
      "concept_ids": [
        "CONCEPT-036"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q085",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team's JWT tokens are valid for 24 hours. When a user's access is revoked, they can still access the system until token expiration. What's the trade-off they accepted?",
      "options": [
        "Stateless authentication simplicity versus immediate revocation capability",
        "Longer tokens are more secure",
        "JWT doesn't support revocation",
        "24 hours is the minimum"
      ],
      "correct_answer": "Stateless authentication simplicity versus immediate revocation capability",
      "explanation": "JWT's stateless nature means no central session to invalidate. Short-lived tokens or token blacklists (adding state) are needed for quick revocation.",
      "concept_ids": [
        "CONCEPT-059"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q086",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A company uses Consumer-Driven Contract Testing. A provider team wants to add a required field to an API response. Existing consumer tests would break. What should happen?",
      "options": [
        "Coordinate with consumers first, update contracts, then make the change",
        "Make the change and let consumers adapt",
        "Delete the contract tests",
        "Add the field as optional instead"
      ],
      "correct_answer": "Coordinate with consumers first, update contracts, then make the change",
      "explanation": "Contract tests exist to catch breaking changes. The failing test is working correctly - it detected a breaking change. Consumers must be coordinated before changes.",
      "concept_ids": [
        "CONCEPT-083"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q087",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "An API Gateway handles authentication, rate limiting, and routing. Response times have increased by 50ms. Where is the latency coming from?",
      "options": [
        "The additional network hop and processing at the gateway layer",
        "Database queries",
        "Service processing",
        "Network cables"
      ],
      "correct_answer": "The additional network hop and processing at the gateway layer",
      "explanation": "API Gateways add latency through the extra network hop and processing for features like auth and rate limiting. This is a known trade-off for centralized concerns.",
      "concept_ids": [
        "CONCEPT-006"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q088",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A team implements throttling that returns HTTP 429 when rate limits are exceeded. Their mobile app crashes when receiving 429s. What did the mobile team do wrong?",
      "options": [
        "Failed to implement proper error handling for rate limit responses",
        "Server-side throttling is wrong",
        "HTTP 429 is not a valid status code",
        "Mobile apps shouldn't be throttled"
      ],
      "correct_answer": "Failed to implement proper error handling for rate limit responses",
      "explanation": "Clients must handle throttling responses (429) gracefully, typically implementing backoff and retry. Crashing indicates missing error handling for this expected scenario.",
      "concept_ids": [
        "CONCEPT-011"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q089",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team's Kubernetes pods keep getting OOMKilled. They set memory limits equal to requests. What's a likely cause?",
      "options": [
        "The application actually needs more memory than the limit allows",
        "Limits should be higher than requests",
        "Kubernetes has a bug",
        "OOMKilled is normal"
      ],
      "correct_answer": "The application actually needs more memory than the limit allows",
      "explanation": "When limits equal requests, the pod gets exactly that memory. OOMKilled means the app tried to use more. Either increase limits or fix memory leaks.",
      "concept_ids": [
        "CONCEPT-025"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q090",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A Saga uses orchestration. The orchestrator becomes a bottleneck and single point of failure. How can this be mitigated while keeping orchestration benefits?",
      "options": [
        "Run multiple orchestrator instances with leader election or partitioned sagas",
        "Switch to choreography",
        "Use bigger servers",
        "Remove the Saga pattern"
      ],
      "correct_answer": "Run multiple orchestrator instances with leader election or partitioned sagas",
      "explanation": "Multiple orchestrator instances with leader election or saga partitioning by key provide scalability and redundancy while maintaining orchestration's clear process visibility.",
      "concept_ids": [
        "CONCEPT-002",
        "CONCEPT-047"
      ],
      "sources": [
        "microservices.io",
        "Azure Patterns"
      ]
    },
    {
      "id": "Q091",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team's post-mortem after an outage focuses on who made the mistake that caused it. Engineers are hesitant to admit errors. What's wrong with this approach?",
      "options": [
        "Blame culture discourages honest reporting and prevents learning about systemic issues",
        "Someone should be blamed",
        "Post-mortems should be shorter",
        "Only major outages need post-mortems"
      ],
      "correct_answer": "Blame culture discourages honest reporting and prevents learning about systemic issues",
      "explanation": "Blameless post-mortems focus on systemic improvements. Blame culture makes people hide mistakes, preventing the organization from learning and improving.",
      "concept_ids": [
        "CONCEPT-070"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q092",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A monolith uses a library for tax calculation. When migrating to microservices, should tax calculation be a separate service?",
      "options": [
        "Not necessarily - if it's a pure calculation with no state, a shared library may be more appropriate",
        "Yes, everything should be a service",
        "No, always use libraries",
        "Depends on the programming language"
      ],
      "correct_answer": "Not necessarily - if it's a pure calculation with no state, a shared library may be more appropriate",
      "explanation": "Pure functions with no state don't require the overhead of a service. The Microservice Premium should be justified - not everything needs to be a service.",
      "concept_ids": [
        "CONCEPT-060"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q093",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A sharded database uses customer ID for sharding. A new requirement needs queries across all customers. What challenge does this create?",
      "options": [
        "Cross-shard queries require scatter-gather across all shards, significantly impacting performance",
        "Sharding prevents all cross-customer queries",
        "Customer ID was wrong for sharding",
        "Add more shards"
      ],
      "correct_answer": "Cross-shard queries require scatter-gather across all shards, significantly impacting performance",
      "explanation": "Sharding optimizes queries within a shard. Cross-shard queries require scatter-gather across all shards, which is expensive. This is a key sharding trade-off.",
      "concept_ids": [
        "CONCEPT-048"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q094",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "An application serves global users. US users report fast response times while EU users report slowness. What's the most likely architectural issue?",
      "options": [
        "Application deployed only in US region, causing high latency for EU users",
        "EU has slower internet",
        "EU users have older devices",
        "Time zone issues"
      ],
      "correct_answer": "Application deployed only in US region, causing high latency for EU users",
      "explanation": "Single-region deployment creates high latency for distant users. Multi-region deployment or CDN for static content would reduce EU latency.",
      "concept_ids": [
        "CONCEPT-032"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q095",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A CI pipeline runs all tests on every commit, taking 45 minutes. Developers push directly to main because feature branch testing is too slow. What's the impact?",
      "options": [
        "Broken commits reach main more often because fast feedback loop is lost",
        "Testing becomes more thorough",
        "Deployment is faster",
        "This is the correct approach"
      ],
      "correct_answer": "Broken commits reach main more often because fast feedback loop is lost",
      "explanation": "Slow CI breaks the fast feedback loop that makes CI effective. Developers bypassing it means broken code reaches main, defeating CI's purpose.",
      "concept_ids": [
        "CONCEPT-052"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q096",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A Kubernetes CRD needs validation that field A must be less than field B. JSON Schema can't express this. What's the best solution without webhooks?",
      "options": [
        "CEL validation expressions in the CRD schema using x-kubernetes-validations",
        "Remove the validation requirement",
        "Use a separate validation service",
        "Implement in the controller only"
      ],
      "correct_answer": "CEL validation expressions in the CRD schema using x-kubernetes-validations",
      "explanation": "Kubernetes supports CEL (Common Expression Language) in CRD schemas for complex validation that JSON Schema can't express, without webhook overhead.",
      "concept_ids": [
        "CONCEPT-075"
      ],
      "sources": [
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q097",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A service handles both real-time API requests and batch reporting. During batch jobs, API latency spikes. What pattern would help?",
      "options": [
        "Bulkhead pattern to isolate resources for API vs batch processing",
        "Disable batch processing",
        "Add more servers",
        "Optimize the batch job"
      ],
      "correct_answer": "Bulkhead pattern to isolate resources for API vs batch processing",
      "explanation": "Bulkhead isolates resources (thread pools, connections) so batch processing can't starve API requests. Each has its own resource partition.",
      "concept_ids": [
        "CONCEPT-009"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q098",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "A team is decomposing a monolith into microservices. They're debating between decomposing by business capability vs. by subdomain. What factors should guide this decision?",
      "correct_answer": "Choose decomposition by business capability when: aligning services with what the business does to generate value, team structure mirrors business functions, clear business ownership exists. Choose subdomain decomposition when: applying DDD principles, domain is complex with core/supporting/generic classification, need to identify strategic differentiators. Both approaches often align; key is deep understanding of both business and domain.",
      "explanation": "Both approaches can work; they often overlap. Business capability aligns with organization, subdomain with domain complexity. Success requires business understanding either way.",
      "concept_ids": [
        "CONCEPT-016",
        "CONCEPT-017"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q099",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A gRPC-based microservices system needs to expose APIs to browser-based web clients. gRPC-Web has limitations. What's the typical solution?",
      "options": [
        "API Gateway that translates REST/HTTP to gRPC for internal services",
        "Rewrite all services in REST",
        "Force browsers to use gRPC",
        "Use WebSockets for everything"
      ],
      "correct_answer": "API Gateway that translates REST/HTTP to gRPC for internal services",
      "explanation": "An API Gateway can expose REST/HTTP to external clients while using efficient gRPC for internal service communication, bridging the browser limitation.",
      "concept_ids": [
        "CONCEPT-006",
        "CONCEPT-076"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q100",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team has 500 alert rules. On-call engineers are overwhelmed with notifications, many of which don't require action. What's the most impactful improvement?",
      "options": [
        "Review and reduce alerts to only actionable items, implementing proper alert hierarchy",
        "Add more on-call engineers",
        "Increase alert thresholds everywhere",
        "Disable alerting"
      ],
      "correct_answer": "Review and reduce alerts to only actionable items, implementing proper alert hierarchy",
      "explanation": "Alert fatigue from non-actionable alerts causes real issues to be missed. Reducing to actionable alerts improves signal-to-noise and on-call effectiveness.",
      "concept_ids": [
        "CONCEPT-021"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q101",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Compare the trade-offs between using a synchronous API composition pattern versus CQRS for queries spanning multiple services.",
      "correct_answer": "API Composition: Simpler implementation, real-time data, but increased latency (multiple calls), reduced availability (depends on all services), and complex aggregation logic. CQRS: Optimized read models with faster queries, but eventual consistency, increased complexity (separate read/write models), and data synchronization overhead. Choose API Composition for simpler queries with strong consistency needs. Choose CQRS for complex queries, high read:write ratios, or when eventual consistency is acceptable.",
      "explanation": "The choice depends on consistency requirements, query complexity, read:write ratio, and team capacity to manage CQRS complexity.",
      "concept_ids": [
        "CONCEPT-003",
        "CONCEPT-063"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q102",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team using Kubernetes notices their Service load balancing is uneven - some pods get 80% of traffic. What's likely happening?",
      "options": [
        "Long-lived connections prevent even distribution; need connection draining or service mesh",
        "Kubernetes load balancing is broken",
        "Some pods are faster",
        "Labels are wrong"
      ],
      "correct_answer": "Long-lived connections prevent even distribution; need connection draining or service mesh",
      "explanation": "Kubernetes Service uses connection-level load balancing. Long-lived connections (gRPC, WebSocket) stay on initial pods. Request-level balancing needs service mesh or client-side LB.",
      "concept_ids": [
        "CONCEPT-046"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q103",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A compliance requirement mandates that all data access must be auditable. The team is choosing between application-level audit logging and Event Sourcing. What's Event Sourcing's advantage here?",
      "options": [
        "Event Sourcing provides 100% complete audit trail by design, as all state changes are recorded as events",
        "Event Sourcing is simpler",
        "Application logging is not auditable",
        "Event Sourcing is required for compliance"
      ],
      "correct_answer": "Event Sourcing provides 100% complete audit trail by design, as all state changes are recorded as events",
      "explanation": "Event Sourcing's fundamental design captures every state change as an event, providing a complete audit trail automatically without risk of missing log statements.",
      "concept_ids": [
        "CONCEPT-004"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q104",
      "skill": "analyze",
      "topic": "technical_debt",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team categorizes their technical debt using the Technical Debt Quadrant. Which quadrant represents the worst kind of debt?",
      "options": [
        "Reckless and Inadvertent - 'We didn't know we were taking on debt'",
        "Prudent and Deliberate",
        "Reckless and Deliberate",
        "Prudent and Inadvertent"
      ],
      "correct_answer": "Reckless and Inadvertent - 'We didn't know we were taking on debt'",
      "explanation": "Reckless/Inadvertent debt (not knowing best practices) is worst because the team doesn't even recognize they're accumulating debt, so they can't plan to address it.",
      "concept_ids": [
        "CONCEPT-029"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q105",
      "skill": "analyze",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Explain why refactoring functionality between services is harder than within a monolith, as stated in the MonolithFirst recommendation.",
      "correct_answer": "Within a monolith: refactoring is a local code change with IDE support, tests run together, deployment is atomic, and rollback is simple. Between services: refactoring requires changing APIs (breaking consumers), coordinating deployments, maintaining backward compatibility during transition, dealing with distributed data movement, handling network failures, and managing potentially different languages/technologies. The distributed nature transforms a code refactoring into an architectural and operational challenge.",
      "explanation": "Service boundaries create hard contracts. Moving functionality requires API versioning, data migration, coordinated deployments, and handling distributed system challenges.",
      "concept_ids": [
        "CONCEPT-027",
        "CONCEPT-060"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q106",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team implemented Queue-Based Load Leveling for their batch processing system. The queue keeps growing even during low-traffic periods. What's the likely issue?",
      "options": [
        "Consumer processing rate is lower than average message production rate",
        "The queue is too small",
        "Messages are too large",
        "The pattern is wrong for batch processing"
      ],
      "correct_answer": "Consumer processing rate is lower than average message production rate",
      "explanation": "If the queue grows continuously, consumers can't keep up with producers even at average rates. Either add consumers or optimize processing.",
      "concept_ids": [
        "CONCEPT-012"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q107",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team implements health checks that return 200 if the process is running. Production routing sends traffic to instances that can't connect to their database. What's wrong?",
      "options": [
        "Health checks should verify actual ability to handle requests, including dependencies",
        "Health checks shouldn't check databases",
        "The load balancer is broken",
        "200 is the wrong status code"
      ],
      "correct_answer": "Health checks should verify actual ability to handle requests, including dependencies",
      "explanation": "Health checks must reflect actual ability to serve traffic, including critical dependencies like databases. A process running but unable to query data isn't healthy.",
      "concept_ids": [
        "CONCEPT-022"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q108",
      "skill": "analyze",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "mcq",
      "question": "A team uses blue-green deployment. Their database schema changed in the green environment. After switching traffic, they need to rollback. Why is this problematic?",
      "options": [
        "Database schema changes may not be backward compatible, preventing rollback to blue",
        "Blue-green doesn't support databases",
        "Rollback is always instant",
        "Schema changes are automatic"
      ],
      "correct_answer": "Database schema changes may not be backward compatible, preventing rollback to blue",
      "explanation": "Blue-green works well for stateless services. Database schema migrations can break backward compatibility, making the 'blue' version unable to work with the new schema.",
      "concept_ids": [
        "CONCEPT-054"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q109",
      "skill": "analyze",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "A team stores configuration in environment variables. They need to update a config value across 50 service instances. What's the operational challenge?",
      "options": [
        "Environment variable changes require restarting all instances, causing potential downtime",
        "Environment variables can't be changed",
        "50 instances is too many",
        "Configuration should be in code"
      ],
      "correct_answer": "Environment variable changes require restarting all instances, causing potential downtime",
      "explanation": "Environment variables are read at startup. Changes require restarts or rolling deployments. An external configuration store enables runtime updates without restarts.",
      "concept_ids": [
        "CONCEPT-081"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q110",
      "skill": "analyze",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "free_response",
      "question": "A system needs to handle a flash sale event where traffic might spike 100x for 1 hour. Analyze the trade-offs between pre-provisioning capacity versus auto-scaling.",
      "correct_answer": "Pre-provisioning: Guaranteed capacity for spike, no scaling lag, but expensive for idle time (99%+ of hours), wastes resources. Auto-scaling: Cost-effective for normal operations, but scaling lag may miss initial spike, cold starts affect performance, may not scale fast enough for 100x spike, need to test actual scaling behavior. Hybrid approach often best: pre-scale some capacity before known event, enable aggressive auto-scaling for additional demand, scale down afterward. For known events, pre-provisioning often wins on reliability.",
      "explanation": "Known, extreme spikes often justify pre-provisioning cost. Unknown spikes need auto-scaling. The 100x magnitude and known timing favor pre-provisioning here.",
      "concept_ids": [
        "CONCEPT-040",
        "CONCEPT-039"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q111",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a payment processing system for an e-commerce platform. Requirements: 99.99% availability, transactions must not be lost, exactly-once payment processing, support 10,000 TPS at peak. The payment gateway is external and occasionally slow (up to 30s). Describe your architecture including patterns used.",
      "correct_answer": "Architecture: 1) API layer with rate limiting and circuit breaker for payment gateway. 2) Transactional Outbox pattern to atomically record payment intent and queue message. 3) Idempotent payment processor with idempotency keys for exactly-once semantics. 4) Saga with orchestration for multi-step payment flow (reserve funds, process payment, update order). 5) Bulkhead isolation for payment types. 6) Event sourcing for payment audit trail. 7) Queue-based load leveling with competing consumers to handle 10K TPS. 8) Multi-AZ deployment for 99.99%. 9) Timeout + async completion for slow gateway calls with webhook callbacks. Key patterns: Circuit Breaker, Transactional Outbox, Saga, Bulkhead, Idempotent Consumer.",
      "explanation": "Payment systems require careful handling of failures, exactly-once semantics, and high availability. This design addresses each requirement with appropriate patterns.",
      "concept_ids": [
        "CONCEPT-001",
        "CONCEPT-002",
        "CONCEPT-009",
        "CONCEPT-019",
        "CONCEPT-024"
      ],
      "sources": [
        "microservices.io",
        "Azure Patterns"
      ]
    },
    {
      "id": "Q112",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "You're tasked with decomposing a 5-year-old monolithic CRM system with 300K lines of code into microservices. The system handles: customer management, sales pipeline, support tickets, reporting, and email campaigns. Design a migration strategy and initial service boundaries.",
      "correct_answer": "Strategy: Strangler Fig pattern for incremental migration. Phase 1: Deploy facade/API Gateway in front of monolith. Phase 2: Extract low-risk, high-value services first. Suggested boundaries based on business capabilities: 1) Customer Service (core customer data, CRUD), 2) Sales Service (pipeline, opportunities, forecasting), 3) Support Service (tickets, SLAs, escalation), 4) Campaign Service (email marketing, automation), 5) Reporting Service (CQRS read models, aggregations). Anti-Corruption Layer between new services and monolith. Database per Service gradually, starting with new features. Shared data initially via API calls or event-driven updates. Migration order: Campaign (low coupling) -> Support -> Sales -> Customer (highest risk, most dependencies) -> Reporting (aggregate view).",
      "explanation": "Strangler Fig with business capability decomposition reduces risk. Start with loosely coupled services, build team experience, then tackle tightly coupled core.",
      "concept_ids": [
        "CONCEPT-008",
        "CONCEPT-016",
        "CONCEPT-015",
        "CONCEPT-005"
      ],
      "sources": [
        "Martin Fowler",
        "microservices.io"
      ]
    },
    {
      "id": "Q113",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a disaster recovery strategy for a critical financial application. Requirements: RTO of 4 hours, RPO of 15 minutes, budget constraints prevent active-active in multiple regions. Which approach best fits?",
      "options": [
        "Warm standby in secondary region with continuous replication and automated failover",
        "Daily backups to another region",
        "Active-active multi-region deployment",
        "Cold standby with weekly backups"
      ],
      "correct_answer": "Warm standby in secondary region with continuous replication and automated failover",
      "explanation": "Warm standby meets the 4-hour RTO (quick spinup) and 15-minute RPO (continuous replication) without the cost of active-active. Cold standby is too slow for 4-hour RTO.",
      "concept_ids": [
        "CONCEPT-033",
        "CONCEPT-034"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q114",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design an order fulfillment system that coordinates: inventory reservation, payment processing, warehouse picking, and shipping. Each step involves different services. Payment and inventory can fail. Design the transaction management approach.",
      "correct_answer": "Use Saga pattern with orchestration. Orchestrator: Order Fulfillment Saga Coordinator. Steps: 1) Reserve Inventory (compensate: release inventory), 2) Process Payment (compensate: refund), 3) Create Pick List (compensate: cancel pick), 4) Initiate Shipping (compensate: cancel shipment). Implementation: Saga orchestrator as separate service storing saga state. Each step uses Transactional Outbox for reliable messaging. Idempotency keys for all operations. Timeout handling with status polling for long operations. Saga state machine: PENDING -> INVENTORY_RESERVED -> PAYMENT_PROCESSED -> PICKING -> SHIPPED. On failure at any step, execute compensating transactions in reverse order. Consider: Inventory reservation timeout, payment webhook callbacks, manual intervention for stuck sagas.",
      "explanation": "Orchestrated saga provides clear flow visibility for order fulfillment. Compensating transactions handle failures at each step.",
      "concept_ids": [
        "CONCEPT-002",
        "CONCEPT-079",
        "CONCEPT-080",
        "CONCEPT-019"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q115",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a logging and monitoring strategy for 50 microservices. Requirements: centralized view, request tracing across services, alerting on SLO violations, 30-day log retention. Which stack best fits?",
      "options": [
        "ELK for logs, Prometheus/Grafana for metrics, Jaeger for tracing, with correlation IDs",
        "Console.log in each service",
        "Centralized relational database for all logs",
        "Separate logging per service without aggregation"
      ],
      "correct_answer": "ELK for logs, Prometheus/Grafana for metrics, Jaeger for tracing, with correlation IDs",
      "explanation": "This stack provides the three pillars of observability. ELK handles log aggregation, Prometheus/Grafana enables SLO alerting, Jaeger provides distributed tracing. Correlation IDs tie them together.",
      "concept_ids": [
        "CONCEPT-020",
        "CONCEPT-021",
        "CONCEPT-085"
      ],
      "sources": [
        "microservices.io",
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q116",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design API versioning strategy for a public API with 500 enterprise customers. Requirements: 6-month deprecation window, clear migration path, support parallel versions, minimize customer disruption. Include versioning approach and deprecation process.",
      "correct_answer": "Versioning approach: URL path versioning (/v1/, /v2/) for clarity and routing simplicity. Process: 1) New version development in parallel with current version. 2) Announce deprecation with 6-month countdown in API responses (Sunset header, deprecation warnings). 3) Provide migration guides and SDK updates. 4) Analytics to track version usage per customer. 5) Direct outreach to customers still on deprecated versions. 6) Grace period extensions for strategic customers. 7) Final deprecation with clear cutoff date. Technical: API Gateway routes by version, shared backend services where compatible, version-specific adapters where needed. Documentation: Changelog, migration guide, version comparison, sunset timeline. Communication: Email announcements, developer portal notices, API response warnings.",
      "explanation": "URL versioning is most explicit for enterprise customers. The deprecation process balances business needs with technical evolution.",
      "concept_ids": [
        "CONCEPT-057",
        "CONCEPT-068"
      ],
      "sources": [
        "ADRs",
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q117",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a caching strategy for a product catalog service. Requirements: 1M products, 100K reads/minute, updates every 5 minutes from suppliers, 99.9% cache hit rate target. Products have images (10MB each) and metadata (1KB). Which approach?",
      "options": [
        "Redis for metadata cache-aside, CDN for images, async cache invalidation on updates",
        "Cache everything in Redis including images",
        "No caching, scale the database",
        "Browser caching only"
      ],
      "correct_answer": "Redis for metadata cache-aside, CDN for images, async cache invalidation on updates",
      "explanation": "Separate concerns: Redis efficiently caches small metadata, CDN handles large images at edge. Cache-aside with async invalidation handles the 5-minute update cycle appropriately.",
      "concept_ids": [
        "CONCEPT-038"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q118",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a zero-trust security architecture for a microservices platform. Requirements: service-to-service authentication, encryption in transit, fine-grained authorization, audit logging, no network-perimeter-based trust.",
      "correct_answer": "Architecture components: 1) Service Mesh (Istio) for automatic mTLS between all services, 2) SPIFFE/SPIRE for service identity and certificate management, 3) JWT tokens with short expiry for service-to-service auth, 4) OPA (Open Policy Agent) for fine-grained authorization policies, 5) Centralized identity provider (Auth0/Keycloak) for user auth, 6) API Gateway for external traffic with authentication, 7) Network policies limiting pod-to-pod communication, 8) Audit logging of all authentication/authorization decisions. Key principles: Never trust network location, verify every request, least privilege by default, assume breach mentality, encrypt everything. Implementation: All services require valid mTLS certificates, authorization checked at each service, all access decisions logged to SIEM.",
      "explanation": "Zero-trust requires multiple layers: identity verification, encryption, authorization, and comprehensive logging without trusting network boundaries.",
      "concept_ids": [
        "CONCEPT-030",
        "CONCEPT-031",
        "CONCEPT-046",
        "CONCEPT-059"
      ],
      "sources": [
        "AWS Well-Architected",
        "microservices.io"
      ]
    },
    {
      "id": "Q119",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "You're designing a multi-tenant SaaS platform. Large enterprise tenants need dedicated resources; small tenants can share resources. Which deployment model?",
      "options": [
        "Hybrid: dedicated infrastructure for enterprise tenants, shared pool for small tenants with tenant-aware routing",
        "Single shared infrastructure for all tenants",
        "Dedicated infrastructure for every tenant regardless of size",
        "Separate codebase per tenant"
      ],
      "correct_answer": "Hybrid: dedicated infrastructure for enterprise tenants, shared pool for small tenants with tenant-aware routing",
      "explanation": "Hybrid balances cost (shared for small tenants) with enterprise requirements (dedicated for large tenants). Routing layer directs traffic appropriately.",
      "concept_ids": [
        "CONCEPT-009"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q120",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design an event-driven architecture for a ride-sharing application. Key events: ride requested, driver assigned, ride started, ride completed, payment processed. Ensure reliability and handle driver no-shows.",
      "correct_answer": "Event-driven design: Event Store (Kafka) as source of truth. Domain Events: RideRequested, DriverAssigned, RideStarted, RideCompleted, PaymentProcessed, DriverNoShow. Services: 1) Ride Service (publishes ride events), 2) Matching Service (consumes RideRequested, publishes DriverAssigned), 3) Driver Service (consumes assignments, publishes location updates), 4) Payment Service (consumes RideCompleted, publishes PaymentProcessed), 5) Notification Service (consumes all events for user notifications). No-show handling: Matching Service sets timeout after DriverAssigned. If RideStarted not received within timeout, publish DriverNoShow, trigger reassignment, update driver reliability score. Reliability: Event Sourcing for ride state, Transactional Outbox for publishing, Idempotent consumers, Dead letter queue for failed processing. CQRS: Read models for real-time dashboards (driver locations, pending rides).",
      "explanation": "Event-driven fits ride-sharing's reactive nature. Timeout-based no-show detection with event-driven reassignment provides reliable handling.",
      "concept_ids": [
        "CONCEPT-004",
        "CONCEPT-023",
        "CONCEPT-062",
        "CONCEPT-003"
      ],
      "sources": [
        "microservices.io",
        "Martin Fowler"
      ]
    },
    {
      "id": "Q121",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a CI/CD pipeline for a microservices team. Requirements: 10 services in a monorepo, independent deployments, automated testing, deployment to Kubernetes. Which approach?",
      "options": [
        "Selective builds based on changed paths, service-specific pipelines, GitOps with ArgoCD for deployment",
        "Single pipeline that builds and deploys everything on every change",
        "Manual deployments after testing",
        "Branch per service with merge to main for deployment"
      ],
      "correct_answer": "Selective builds based on changed paths, service-specific pipelines, GitOps with ArgoCD for deployment",
      "explanation": "Path-based selective builds handle monorepo efficiency. Service-specific pipelines enable independent deployments. ArgoCD provides GitOps-based Kubernetes deployment.",
      "concept_ids": [
        "CONCEPT-052",
        "CONCEPT-053",
        "CONCEPT-036"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q122",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a rate limiting strategy for a multi-tier API. Requirements: 1000 req/min for free tier, 10000 req/min for paid tier, 100000 req/min for enterprise. Must prevent abuse while allowing legitimate bursts.",
      "correct_answer": "Implementation: Token Bucket algorithm with per-tier configuration. Free: 1000 tokens/minute, burst capacity 100. Paid: 10000 tokens/minute, burst capacity 1000. Enterprise: 100000 tokens/minute, burst capacity 10000. Architecture: 1) Distributed rate limiter using Redis for token counts, 2) API Gateway enforces limits before request processing, 3) Rate limit headers in responses (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset), 4) HTTP 429 response with Retry-After header when exceeded, 5) Per-API-key tracking for multi-key enterprise accounts, 6) Sliding window for fair distribution, 7) Graceful degradation: read-only access when write limits exceeded, 8) Alerting when customers approach limits for upsell opportunity. Anti-abuse: IP-based limits as backstop, anomaly detection for unusual patterns.",
      "explanation": "Token bucket allows bursts while enforcing average rates. Distributed implementation with Redis ensures consistency across instances.",
      "concept_ids": [
        "CONCEPT-011"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q123",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a notification service for a social media platform. Requirements: 1M+ users, real-time push, email digest, user preferences, guaranteed delivery for important notifications. Which architecture?",
      "options": [
        "Event-driven with priority queues, notification service subscribing to user events, preference-based routing, dead letter queue for retry",
        "Direct database inserts from all services",
        "Synchronous API calls for each notification",
        "Batch processing only"
      ],
      "correct_answer": "Event-driven with priority queues, notification service subscribing to user events, preference-based routing, dead letter queue for retry",
      "explanation": "Event-driven decouples notification from source services. Priority queues ensure important notifications aren't delayed. Preference routing respects user choices.",
      "concept_ids": [
        "CONCEPT-023",
        "CONCEPT-012"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q124",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a data mesh architecture for a large enterprise with multiple business domains: Sales, Marketing, Finance, Operations. Each domain should own their data products. Include governance and interoperability considerations.",
      "correct_answer": "Data Mesh Design: Domain-oriented ownership: Each domain (Sales, Marketing, Finance, Ops) owns data products as first-class citizens. Infrastructure: 1) Self-serve data platform providing compute, storage, and catalog capabilities, 2) Federated computational governance with automated policy enforcement, 3) Standardized interfaces (APIs, event streams) for data product consumption. Data Products per domain: Sales (leads, opportunities, forecasts), Marketing (campaigns, attribution, segments), Finance (revenue, costs, budgets), Ops (inventory, fulfillment, logistics). Interoperability: 1) Standard metadata schemas across domains, 2) Data contracts with SLOs, 3) Cross-domain access via governed APIs, 4) Central catalog for discovery. Governance: Automated quality checks, data lineage tracking, access policies, retention compliance. Implementation: Start with one domain, prove model, expand. Anti-pattern to avoid: central data team owning all data.",
      "explanation": "Data mesh applies domain-driven design to data, with domains owning their data products. Self-serve platform and federated governance enable scaling.",
      "concept_ids": [
        "CONCEPT-017",
        "CONCEPT-018",
        "CONCEPT-065"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q125",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design auto-scaling for an e-commerce platform expecting 10x traffic during flash sales (known events) and 3x spikes during normal operation (unknown events). Which scaling configuration?",
      "options": [
        "Scheduled scaling for known events, target tracking for normal operation, with pre-warming for flash sales",
        "Manual scaling before flash sales",
        "Only reactive auto-scaling based on CPU",
        "Fixed capacity for peak load"
      ],
      "correct_answer": "Scheduled scaling for known events, target tracking for normal operation, with pre-warming for flash sales",
      "explanation": "Scheduled scaling pre-provisions for known 10x events. Target tracking handles unknown 3x spikes reactively. Pre-warming ensures instances are ready for traffic.",
      "concept_ids": [
        "CONCEPT-040"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q126",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a search service for a marketplace with 50M products, real-time inventory updates, personalized results, and fuzzy matching. Include indexing strategy and consistency approach.",
      "correct_answer": "Architecture: 1) Elasticsearch cluster for search indexing with product data, 2) Event-driven updates via Kafka for real-time inventory changes, 3) Product Service as source of truth for product data, 4) CDC (Change Data Capture) for database-to-Kafka sync, 5) Personalization Service enriching queries with user preferences. Indexing Strategy: Product index with denormalized data (title, description, categories, price, availability), Inventory as near-real-time field updated via lightweight events, User preference index for personalization signals. Consistency: Eventual consistency acceptable for search (stale inventory OK for few seconds), Critical inventory checks at checkout with source of truth. Fuzzy matching: Elasticsearch n-gram tokenizer, phonetic analysis, synonym expansion, spell correction. Query flow: API Gateway -> Query Parser -> Personalization enrichment -> Elasticsearch -> Results ranking -> Response. Scaling: Shard by category, read replicas for query load.",
      "explanation": "Elasticsearch handles search requirements. Event-driven updates provide near-real-time inventory. Eventual consistency is acceptable for search with checkout validation.",
      "concept_ids": [
        "CONCEPT-042",
        "CONCEPT-003"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q127",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design an authentication system for a platform with web app, mobile app, and third-party API access. Requirements: SSO, MFA support, token refresh, and API keys for developers.",
      "options": [
        "OAuth 2.0 with OIDC for user auth, separate API key management for developers, JWT with refresh tokens, MFA via TOTP",
        "Session cookies for everything",
        "API keys for all access types",
        "Basic authentication with long-lived tokens"
      ],
      "correct_answer": "OAuth 2.0 with OIDC for user auth, separate API key management for developers, JWT with refresh tokens, MFA via TOTP",
      "explanation": "OAuth 2.0/OIDC is the standard for user authentication with SSO. API keys suit developer/machine access. JWTs enable stateless verification with refresh for longevity.",
      "concept_ids": [
        "CONCEPT-058",
        "CONCEPT-059"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q128",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design service boundaries for a banking application covering: accounts, transactions, loans, cards, and fraud detection. Consider data ownership, transaction boundaries, and regulatory requirements.",
      "correct_answer": "Service Boundaries: 1) Account Service: customer accounts, balances, account lifecycle (data owner: account data), 2) Transaction Service: deposits, withdrawals, transfers (orchestrates cross-account), 3) Loan Service: loan applications, disbursements, repayments (separate lifecycle), 4) Card Service: card issuance, limits, PIN management (security isolation), 5) Fraud Service: real-time fraud scoring, pattern detection (receives events from all). Transaction boundaries: Cross-account transfers use Saga (reserve source -> credit destination -> confirm source). Strong consistency within service, eventual between services. Regulatory: Audit logging in all services, immutable transaction history (event sourcing consideration for Transaction Service), data residency per account region. Data ownership: Each service owns its domain data. Shared reference data (customer profile) accessed via API with caching. Event-driven: All services publish domain events for fraud detection consumption.",
      "explanation": "Banking requires careful service boundaries around data ownership and regulatory needs. Strong consistency within financial transactions, saga for cross-service.",
      "concept_ids": [
        "CONCEPT-016",
        "CONCEPT-005",
        "CONCEPT-002"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q129",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a secrets management strategy for 100 microservices on Kubernetes. Requirements: automatic rotation, audit trail, development/production isolation, emergency revocation.",
      "options": [
        "HashiCorp Vault with Kubernetes auth, dynamic secrets, audit logging, and namespace isolation",
        "Kubernetes Secrets with GitOps",
        "Environment variables in deployment manifests",
        "Hardcoded in application config"
      ],
      "correct_answer": "HashiCorp Vault with Kubernetes auth, dynamic secrets, audit logging, and namespace isolation",
      "explanation": "Vault provides dynamic secrets with automatic rotation, comprehensive audit logging, and fine-grained access control. Kubernetes auth integrates with pod identity.",
      "concept_ids": [
        "CONCEPT-030",
        "CONCEPT-081"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q130",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a real-time analytics dashboard for an IoT platform. Requirements: 100K devices sending data every second, 5-second latency for dashboard updates, historical queries for last 30 days, anomaly alerts.",
      "correct_answer": "Architecture: Lambda architecture combining real-time and batch. Ingestion: Kafka for device data ingestion (partitioned by device ID). Stream processing: Apache Flink/Spark Streaming for real-time aggregations (5-second windows). Speed layer: Time-series database (InfluxDB/TimescaleDB) for recent hot data (7 days). Batch layer: Data lake (S3/HDFS) with Spark batch jobs for historical aggregations. Serving: Pre-computed dashboards from both layers, merged at query time. Anomaly detection: Flink stream processor with ML model, alerts via Kafka to Notification Service. Dashboard: WebSocket connection for real-time updates, REST API for historical queries. Scaling: Kafka partitions = device count / 1000, Flink parallelism based on throughput, time-series DB sharded by time. Data model: device_id, timestamp, metrics map, metadata. Retention: Hot (7 days, high resolution) -> Warm (30 days, downsampled) -> Cold (archive, highly compressed).",
      "explanation": "Lambda architecture handles both real-time and historical requirements. Time-series DB optimized for IoT data patterns. Stream processing enables low-latency dashboards.",
      "concept_ids": [
        "CONCEPT-023"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q131",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a data backup strategy for a globally distributed application with databases in 3 regions. Requirements: cross-region recovery, 1-hour RPO, 4-hour RTO, encryption at rest.",
      "options": [
        "Continuous replication to central backup region, point-in-time recovery enabled, encrypted snapshots, tested restore procedures",
        "Daily backups to same region",
        "Manual exports weekly",
        "No backups, rely on replication"
      ],
      "correct_answer": "Continuous replication to central backup region, point-in-time recovery enabled, encrypted snapshots, tested restore procedures",
      "explanation": "Continuous replication meets 1-hour RPO. Central backup enables cross-region recovery. Point-in-time recovery provides flexibility. Tested procedures ensure 4-hour RTO.",
      "concept_ids": [
        "CONCEPT-033",
        "CONCEPT-034"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q132",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a content moderation system for a user-generated content platform. Requirements: text, image, and video moderation; human review workflow; appeals process; audit trail; sub-100ms for text, 24h SLA for video.",
      "correct_answer": "Architecture: Event-driven with content type routing. Services: 1) Content Ingestion Service: receives uploads, publishes ContentSubmitted events, 2) Text Moderation Service: ML-based text analysis, sync for sub-100ms requirement, 3) Image Moderation Service: async with ML + hash-based matching, 4) Video Moderation Service: queue-based with 24h SLA, can use external provider, 5) Human Review Service: workflow for escalated content, 6) Appeals Service: resubmission workflow with different reviewers, 7) Audit Service: immutable event log of all decisions. Flow: Content submitted -> type-specific moderation -> ML scoring -> auto-approve/reject/escalate based on confidence. Human review for medium confidence and appeals. Escalation queue with priority. Audit: Event sourcing for complete decision history, who decided, when, why. Metrics: Moderation latency, false positive/negative rates, reviewer queue depth. Scaling: Text sync in-memory, Image/Video via queue with competing consumers.",
      "explanation": "Different content types have different latency requirements. ML for automation, human review for edge cases. Event sourcing provides audit trail.",
      "concept_ids": [
        "CONCEPT-012",
        "CONCEPT-004",
        "CONCEPT-051"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q133",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design message handling for an order system where processing must happen exactly once. Messages may be redelivered due to consumer failures. Which approach ensures correctness?",
      "options": [
        "Idempotent consumer with message ID tracking in the same transaction as business logic",
        "Rely on exactly-once delivery from message broker",
        "Process all messages regardless of duplicates",
        "Manual deduplication by operators"
      ],
      "correct_answer": "Idempotent consumer with message ID tracking in the same transaction as business logic",
      "explanation": "Most message systems provide at-least-once delivery. Idempotent consumers with transactional ID tracking ensure exactly-once processing semantics.",
      "concept_ids": [
        "CONCEPT-024"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q134",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a cost optimization strategy for a cloud-native startup spending $200K/month on AWS. They have production, staging, and development environments. Workloads are variable with peaks on weekdays.",
      "correct_answer": "Analysis and Strategy: 1) Environment scheduling: Shut down dev/staging outside business hours (save ~60% on non-prod), 2) Right-sizing: Analyze utilization, downsize over-provisioned instances (typical 20-30% savings), 3) Reserved capacity: Commit to 1-year Reserved Instances for steady-state production workloads (30-40% savings), 4) Spot instances: Use for fault-tolerant batch processing, CI/CD builds (60-90% savings), 5) Storage tiering: S3 lifecycle policies moving old data to Glacier (90% savings on archive), 6) Data transfer: Review cross-AZ/region traffic, consolidate where possible, 7) Right-size databases: RDS instance sizing, consider Aurora Serverless for variable loads, 8) Container optimization: Pod right-sizing, cluster autoscaling, Spot for non-critical pods. Implementation: Cost allocation tags, weekly cost review, per-team budgets, automated anomaly alerts. Expected outcome: 40-50% reduction ($80-100K/month savings).",
      "explanation": "Multi-pronged approach: scheduling for non-prod, reservations for stable prod, spot for batch, storage tiering, right-sizing everywhere.",
      "concept_ids": [
        "CONCEPT-038",
        "CONCEPT-039"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q135",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design an SLO framework for a microservices platform. You need to define SLOs, measure SLIs, and create error budgets. Which approach is most comprehensive?",
      "options": [
        "Define latency (p99) and availability SLOs per service, measure with distributed tracing, calculate error budget burn rate for release decisions",
        "Track only uptime percentage",
        "Use average latency for all measurements",
        "Measure only customer complaints"
      ],
      "correct_answer": "Define latency (p99) and availability SLOs per service, measure with distributed tracing, calculate error budget burn rate for release decisions",
      "explanation": "P99 latency captures tail experience. Per-service SLOs enable accountability. Error budgets tie reliability to deployment velocity.",
      "concept_ids": [
        "CONCEPT-021",
        "CONCEPT-020"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q136",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a multi-region active-active deployment for a global SaaS application. Requirements: local latency for users in each region, data sovereignty compliance, conflict resolution for concurrent updates, 99.99% global availability.",
      "correct_answer": "Architecture: Active-active deployment in 3 regions (US, EU, APAC). Traffic routing: Global load balancer (CloudFront/CloudFlare) routing to nearest region. Data strategy: Per-region primary for user data (data sovereignty), async replication with conflict resolution. Conflict resolution: Last-writer-wins with vector clocks for non-critical data, application-level merge for business-critical data, CRDTs where applicable. Database: Multi-region capable database (CockroachDB, Spanner, or managed solutions). Regional isolation: User data stays in region of registration. Cross-region: Shared reference data replicated async. Failure handling: Region failure -> traffic rerouted to next nearest region within seconds. Deployment: Independent deployments per region, feature flags for regional rollout. Monitoring: Global dashboard with per-region health, cross-region latency tracking. Availability math: 3 independent regions at 99.9% each = 99.9999% combined (if any region can serve any user).",
      "explanation": "Active-active requires careful conflict resolution. Data sovereignty addressed via regional data placement. Global LB provides failover and latency optimization.",
      "concept_ids": [
        "CONCEPT-033",
        "CONCEPT-042"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q137",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design data contracts between services in a microservices architecture. The Order Service needs Customer data from Customer Service. Which approach minimizes coupling while ensuring reliability?",
      "options": [
        "Customer Service publishes events with required data, Order Service maintains local cache updated via events",
        "Order Service directly queries Customer database",
        "Shared Customer library across all services",
        "Synchronous API call on every order"
      ],
      "correct_answer": "Customer Service publishes events with required data, Order Service maintains local cache updated via events",
      "explanation": "Event-driven data sharing minimizes runtime coupling. Local cache provides availability. Events define the contract. This is the Data Mesh/event-carried state transfer pattern.",
      "concept_ids": [
        "CONCEPT-005",
        "CONCEPT-062"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q138",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a Kubernetes deployment strategy for a critical service requiring zero-downtime deployments, rollback capability, and gradual traffic shifting. Include health checks and observability.",
      "correct_answer": "Deployment Strategy: Rolling update with canary promotion. Configuration: Deployment with: maxUnavailable: 0 (zero downtime), maxSurge: 25%, minReadySeconds: 60 (stability check), progressDeadlineSeconds: 600. Health checks: livenessProbe (restart if unhealthy), readinessProbe (traffic routing), startupProbe (slow-start apps). Canary: Use Argo Rollouts or Flagger for progressive delivery. Stages: 10% traffic -> metrics check -> 50% -> metrics check -> 100%. Rollback triggers: Error rate > 1%, P99 latency > 500ms, crash loop detected. Observability: Prometheus metrics scraping, custom metrics for business KPIs, Grafana dashboards per deployment, alerts on SLO breach. Rollback: Automatic on health check failure, manual via kubectl rollout undo. Testing: Pre-deployment integration tests, post-deployment smoke tests. Manifests: Separate ConfigMaps/Secrets with checksums for config-triggered rollouts.",
      "explanation": "Rolling updates with careful health checks enable zero-downtime. Canary with automated analysis catches issues before full rollout. Quick rollback minimizes impact.",
      "concept_ids": [
        "CONCEPT-055",
        "CONCEPT-022",
        "CONCEPT-025"
      ],
      "sources": [
        "ADRs",
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q139",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a chaos engineering program for a company new to the practice. They have 20 microservices in production with basic monitoring. What's the best starting approach?",
      "options": [
        "Start with GameDays in staging, simple failure injection (pod kill), graduate to production with small blast radius",
        "Immediately run chaos experiments in production at scale",
        "Only theoretical analysis without actual injection",
        "Chaos engineering only after all services are fully resilient"
      ],
      "correct_answer": "Start with GameDays in staging, simple failure injection (pod kill), graduate to production with small blast radius",
      "explanation": "Start small and safe: staging first, simple failures, build confidence and improve observability before production. Gradually increase scope and complexity.",
      "concept_ids": [
        "CONCEPT-037"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q140",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a recommendation engine for an e-commerce platform. Requirements: personalized product recommendations, new user cold start handling, real-time updates based on browsing, batch model training, A/B testing capability.",
      "correct_answer": "Architecture: 1) Data Collection Layer: Click stream collector via Kafka, user events aggregator, product interaction tracker. 2) Feature Store: User features (purchase history, preferences), Product features (categories, attributes), Real-time features (current session). 3) Model Training: Batch training pipeline (Spark), collaborative filtering + content-based hybrid, daily model updates to S3. 4) Serving Layer: Real-time inference service (low latency), model loaded in memory, feature enrichment at request time. 5) Cold Start Strategy: Content-based recommendations for new users (based on similar products viewed), popular items fallback, quick preference capture. 6) Real-time Updates: Kafka consumer updating user session features, immediate re-ranking based on browsing. 7) A/B Testing: Feature flag system for model variants, metrics collection per variant, statistical significance calculation, gradual rollout. Flow: Request -> Feature retrieval -> Model inference -> A/B variant selection -> Post-processing (dedup, inventory filter) -> Response. Monitoring: Recommendation click-through rate, conversion rate, diversity metrics.",
      "explanation": "Hybrid approach handles different scenarios. Real-time updates improve relevance. A/B testing enables safe model iteration.",
      "concept_ids": [
        "CONCEPT-056"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q141",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design service communication for an order processing system. Order Service needs to call Payment Service (critical, needs response) and Notification Service (non-critical, can fail). Which communication patterns?",
      "options": [
        "Synchronous call with Circuit Breaker for Payment, async message queue for Notification",
        "Synchronous calls for both with long timeouts",
        "Async message queue for both",
        "Direct database sharing"
      ],
      "correct_answer": "Synchronous call with Circuit Breaker for Payment, async message queue for Notification",
      "explanation": "Critical paths (Payment) need synchronous calls with resilience patterns. Non-critical paths (Notification) benefit from async decoupling - failure doesn't block order.",
      "concept_ids": [
        "CONCEPT-001",
        "CONCEPT-023"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q142",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a database migration strategy for moving from PostgreSQL monolith database to service-owned databases. Current state: 200 tables, 50 services reading/writing to shared database. Goal: Database per Service.",
      "correct_answer": "Migration Strategy: Phase 1 - Preparation: Catalog all tables and owning services, identify table dependencies, document cross-service queries. Phase 2 - API Layer: Create data access APIs in owning services before moving tables, redirect other services to use APIs instead of direct DB access. Phase 3 - Dual Write: Owning service writes to both old and new database, sync verified by reconciliation jobs. Phase 4 - Cutover: Switch reads to new database, old database becomes read-only, then decommission. Per-table approach: High-priority tables (critical/high-traffic) migrated first with more testing, Low-priority tables in batches. Cross-service queries: Replace with API calls or CQRS read models. Rollback plan: Maintain reverse sync capability until migration stable. Timeline: Expect 12-18 months for 200 tables. Tools: CDC for sync (Debezium), schema migration (Flyway), shadow traffic testing. Risk mitigation: Feature flags for routing, extensive testing, runbooks for issues.",
      "explanation": "Gradual migration with dual-write prevents data loss. API layer first decouples services before physical migration. Long timeline for safety.",
      "concept_ids": [
        "CONCEPT-005",
        "CONCEPT-008"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q143",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a structured error handling strategy for a public API. Errors need to be: actionable for clients, secure (no internal details exposed), consistent across services, and debuggable for operators.",
      "options": [
        "RFC 7807 Problem Details format with error codes, correlation IDs in response and logs, separate internal vs external error messages",
        "Stack traces in API responses",
        "Generic 500 Internal Server Error for all failures",
        "Error codes without descriptions"
      ],
      "correct_answer": "RFC 7807 Problem Details format with error codes, correlation IDs in response and logs, separate internal vs external error messages",
      "explanation": "RFC 7807 provides standard format. Error codes enable client handling. Correlation IDs link client errors to server logs. Separation protects internals.",
      "concept_ids": [
        "CONCEPT-066"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q144",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a distributed locking mechanism for a job scheduler that runs on multiple instances. Requirements: only one instance processes each job, lock must auto-expire if holder crashes, at-least-once processing with idempotency.",
      "correct_answer": "Distributed Lock Design: Lock store: Redis with SETNX or ZooKeeper/etcd for stronger guarantees. Lock acquisition: SETNX with expiry (fencing token pattern for safety). Lock structure: job_id as key, instance_id + timestamp + fencing_token as value. Auto-expiry: TTL on lock (e.g., 5 minutes), heartbeat extends TTL while processing. Fencing token: Monotonic token returned on lock acquisition, passed to downstream operations, storage rejects operations with stale tokens. Algorithm: 1) Try acquire lock with SETNX, 2) If acquired, start heartbeat thread, 3) Process job (pass fencing token), 4) Release lock on completion, 5) On crash, lock expires, job reprocessed by another instance. At-least-once: Failed jobs released for retry. Idempotency: Job processing must be idempotent (use job_id as idempotency key). Considerations: Clock skew for TTL, split-brain with network partition, Redlock for Redis multi-node safety. Monitoring: Lock acquisition latency, lock hold times, contested lock ratio.",
      "explanation": "Fencing tokens prevent split-brain issues. Auto-expiry handles crashes. Idempotency ensures correctness despite at-least-once processing.",
      "concept_ids": [
        "CONCEPT-047",
        "CONCEPT-024"
      ],
      "sources": [
        "Azure Patterns",
        "microservices.io"
      ]
    },
    {
      "id": "Q145",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a testing strategy for microservices. You have 15 services with complex dependencies. Full integration tests take 2 hours. How do you balance test coverage with developer productivity?",
      "options": [
        "Unit tests per service, contract tests for service boundaries, selective integration tests based on changes, full integration in nightly builds",
        "Only unit tests for speed",
        "Only full integration tests for coverage",
        "Manual testing only"
      ],
      "correct_answer": "Unit tests per service, contract tests for service boundaries, selective integration tests based on changes, full integration in nightly builds",
      "explanation": "Testing pyramid: fast unit tests for most coverage, contract tests catch interface breaks, selective integration for changed areas, comprehensive nightly for full validation.",
      "concept_ids": [
        "CONCEPT-083",
        "CONCEPT-052"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q146",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a GitOps-based deployment pipeline for 30 microservices. Requirements: environment promotion (dev -> staging -> prod), automated rollbacks, drift detection, secrets management integration.",
      "correct_answer": "GitOps Architecture: Repository structure: /apps (per-service configs), /base (shared configs), /environments (dev/staging/prod overlays using Kustomize). Tools: ArgoCD for sync, Kustomize for overlays, Sealed Secrets for secret management. Workflow: 1) Developer merges to main -> CI builds image -> updates dev overlay, 2) ArgoCD syncs dev automatically, 3) Promotion: PR from dev to staging overlay, 4) Manual approval for prod promotion. Environment configs: Base configs in /base, environment-specific in /environments/[env]/. Automated rollbacks: ArgoCD sync failure -> auto rollback to previous Git commit, alert to team. Drift detection: ArgoCD continuous sync with auto-heal, alerts on manual changes. Secrets: SealedSecrets or External Secrets Operator syncing from Vault. Monitoring: ArgoCD dashboard, sync status metrics, deployment frequency tracking. Rollback process: Revert commit in Git -> ArgoCD syncs automatically -> service restored. Branch strategy: Single main branch, environment promotion via directory overlays.",
      "explanation": "Git as single source of truth for all environments. Kustomize overlays handle environment differences. ArgoCD provides sync and rollback automation.",
      "concept_ids": [
        "CONCEPT-036",
        "CONCEPT-053"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q147",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a capacity planning approach for a rapidly growing startup. User base doubles every 6 months. Current infrastructure costs are manageable but you need to plan ahead. Which approach?",
      "options": [
        "Trend analysis with 3-6 month projections, auto-scaling configured aggressively, quarterly capacity reviews, load testing at 2x current peak",
        "Wait for performance issues",
        "Over-provision for 5 years of growth",
        "Manual scaling when customers complain"
      ],
      "correct_answer": "Trend analysis with 3-6 month projections, auto-scaling configured aggressively, quarterly capacity reviews, load testing at 2x current peak",
      "explanation": "Trend analysis anticipates needs. Auto-scaling handles variability. Regular reviews adjust plans. Load testing validates capacity before it's needed.",
      "concept_ids": [
        "CONCEPT-038",
        "CONCEPT-040"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q148",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a file upload and processing pipeline. Requirements: support files up to 5GB, virus scanning, format validation, thumbnail generation for images, metadata extraction, resumable uploads.",
      "correct_answer": "Architecture: 1) Upload Service: Receives multipart uploads, supports resumable uploads via pre-signed URLs for direct S3 upload with chunked transfer. 2) Pre-signed URL approach: Client requests upload URL, uploads directly to S3, S3 triggers processing on completion. 3) Processing Pipeline (Step Functions or queue-based): S3 trigger -> Virus Scan Lambda (ClamAV) -> Format Validation -> Route by type -> Image: Thumbnail Generator, Document: Metadata Extractor -> Update File Service with results. 4) Virus scanning: Dedicated scanning instances, quarantine bucket for suspicious files. 5) Resumable: S3 multipart upload with client-tracked parts. 6) Large files: Chunked upload to S3, parallel processing where possible. 7) Error handling: Failed processing -> DLQ -> alert -> manual review. 8) Status tracking: File Service maintains processing status, client polls or receives webhook. Storage: Original files in upload bucket, processed files in serving bucket, thumbnails in CDN-backed bucket. Security: Pre-signed URLs expire quickly, no direct bucket access.",
      "explanation": "Direct-to-S3 upload handles large files efficiently. Event-driven pipeline processes asynchronously. Each step is independent and scalable.",
      "concept_ids": [
        "CONCEPT-050",
        "CONCEPT-012"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q149",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design team structure for a microservices platform with 8 services. Each service should be owned by a team, but you only have 25 engineers. How should teams be organized?",
      "options": [
        "3-4 cross-functional teams, each owning 2-3 related services based on domain boundaries",
        "One team owning all services",
        "8 teams with 3 engineers each",
        "Separate teams for frontend, backend, and DevOps"
      ],
      "correct_answer": "3-4 cross-functional teams, each owning 2-3 related services based on domain boundaries",
      "explanation": "Two-pizza team size (6-8 people) owns related services within a domain. Cross-functional includes all needed skills. Aligns with Conway's Law.",
      "concept_ids": [
        "CONCEPT-043",
        "CONCEPT-016"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q150",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a multi-cluster Kubernetes strategy for a global application. Requirements: failover between clusters, centralized management, consistent policies, workload distribution based on regional user traffic.",
      "correct_answer": "Multi-Cluster Architecture: Cluster topology: Regional clusters (US, EU, APAC) + management cluster. Federation: Kubernetes Federation v2 or Rancher Fleet for multi-cluster management. Traffic routing: Global load balancer (CloudFlare/AWS Global Accelerator) routing based on latency. Cluster management: GitOps with Fleet/ArgoCD multi-cluster sync from single repo. Policy enforcement: OPA Gatekeeper with centralized policy repo, synced to all clusters. Workload distribution: Deployment manifests define regional targets, override for region-specific config. Failover: Health checks at global LB, automatic failover to next nearest healthy cluster, manual override capability. Service mesh: Istio multi-cluster for cross-cluster service discovery and traffic management. Observability: Centralized monitoring (Thanos for metrics federation), unified logging. DR: Velero backups per cluster to central location. Consistency: Base configs in central repo, regional overlays where needed.",
      "explanation": "Federation provides centralized management. GitOps ensures consistency. Global LB handles routing and failover. Centralized policies maintain governance.",
      "concept_ids": [
        "CONCEPT-046",
        "CONCEPT-036"
      ],
      "sources": [
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q151",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design an incident response process for a 24/7 SaaS platform. Team of 20 engineers across 3 time zones. Need: clear escalation, minimal burnout, effective communication.",
      "options": [
        "Follow-the-sun on-call rotation, severity-based escalation matrix, incident commander role, post-incident reviews, runbooks for common issues",
        "Single person on-call 24/7",
        "No on-call, address issues next business day",
        "Entire team responds to every incident"
      ],
      "correct_answer": "Follow-the-sun on-call rotation, severity-based escalation matrix, incident commander role, post-incident reviews, runbooks for common issues",
      "explanation": "Follow-the-sun leverages time zones to avoid 24-hour on-call. Severity-based escalation prevents over-alerting. Incident commander coordinates response. Runbooks enable efficient resolution.",
      "concept_ids": [
        "CONCEPT-070",
        "CONCEPT-066"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q152",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a workflow orchestration system for complex, long-running business processes (e.g., loan application that takes weeks). Requirements: human tasks, conditional routing, SLA tracking, process versioning, audit trail.",
      "correct_answer": "Workflow Engine Design: Core components: Workflow Engine (orchestrator), Task Queue, Timer Service, Human Task Service, Audit Logger. Workflow definition: DSL or BPMN, stored as versioned documents. Execution: Each workflow instance has unique ID, state machine tracks current position. State persistence: Durable storage (PostgreSQL) for workflow state, event sourcing for history. Human tasks: Tasks pushed to assignee queues, UI for task claiming/completion, timeout handling, reassignment. Conditional routing: Expression evaluation engine for branching logic. SLA tracking: Timer Service schedules SLA checks, escalation events on breach. Versioning: Workflow definitions versioned, running instances continue on original version, new instances use latest, migration for long-running if needed. Audit: Every state transition logged with timestamp, actor, decision. Long-running: Workflow sleeps between activities, Timer Service wakes when needed. Integration: Activities call external services via adapters. Technology options: Temporal, Cadence, or Camunda for BPMN support. Monitoring: Dashboard for workflow status, SLA breach alerts, bottleneck identification.",
      "explanation": "Durable workflow engine handles long-running processes. Event sourcing provides audit trail. Versioning allows process evolution without disrupting running instances.",
      "concept_ids": [
        "CONCEPT-002",
        "CONCEPT-004"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q153",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design an integration strategy for a microservices platform that must connect with 20 different external partners. Each partner has different API formats, auth methods, and reliability characteristics.",
      "options": [
        "Integration Service with adapter per partner, circuit breakers per partner, async messaging for non-real-time, standardized internal events",
        "Direct calls from each microservice to partners",
        "Single universal API wrapper",
        "Manual integration scripts"
      ],
      "correct_answer": "Integration Service with adapter per partner, circuit breakers per partner, async messaging for non-real-time, standardized internal events",
      "explanation": "Adapter pattern handles API differences. Circuit breakers isolate partner failures. Async messaging decouples when possible. Internal events standardize consumption.",
      "concept_ids": [
        "CONCEPT-015",
        "CONCEPT-001"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q154",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a feature flag system for a microservices platform. Requirements: per-user targeting, percentage rollouts, kill switch capability, real-time updates, minimal latency impact.",
      "correct_answer": "Feature Flag System: Architecture: Flag configuration service (source of truth), SDKs in each service, edge caching for performance. Flag types: Boolean (on/off), Percentage rollout, User targeting (by ID, attributes), Environment-based. Configuration: Flag definitions in database/config store, admin UI for management, audit log for changes. SDK design: Initialize with config fetch, local cache with TTL, background refresh, fallback to defaults. Evaluation: SDK evaluates locally from cache (sub-ms), targeting rules evaluated in order: user overrides -> percentage hash -> default. Real-time updates: Webhook/SSE push to SDKs on flag change, or short polling (30s) as fallback. Kill switch: High-priority flag type, pushed immediately, SDKs prioritize kill switch updates. Caching: SDK maintains local cache, CDN for initial config fetch, Redis for shared state if needed. Monitoring: Flag evaluation metrics, percentage actual vs configured, latency of evaluation. Cleanup: Alert on old flags, archive unused, remove from code. Tools: LaunchDarkly, Split.io, or build on feature-flag-patterns.",
      "explanation": "Local SDK evaluation minimizes latency. Push updates enable real-time changes. Kill switch has fast path. Gradual rollout via consistent hashing on user ID.",
      "concept_ids": [
        "CONCEPT-056"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q155",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a compliance framework for a healthcare application handling PHI. Requirements: HIPAA compliance, audit logging, encryption, access controls, incident reporting.",
      "options": [
        "Encryption at rest and in transit, role-based access with audit logging, BAA with cloud provider, automated compliance scanning, incident response procedures",
        "Standard application security practices",
        "Encrypt only during transmission",
        "Annual security audit only"
      ],
      "correct_answer": "Encryption at rest and in transit, role-based access with audit logging, BAA with cloud provider, automated compliance scanning, incident response procedures",
      "explanation": "HIPAA requires comprehensive controls: encryption, access controls, auditing, business associate agreements, and documented incident response. Automation ensures ongoing compliance.",
      "concept_ids": [
        "CONCEPT-030",
        "CONCEPT-031"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q156",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a real-time bidding (RTB) system for an ad platform. Requirements: 100ms response time budget, 100K QPS, bid price prediction, budget pacing, winner notification.",
      "correct_answer": "RTB Architecture: Request flow: Ad request -> Load balancer -> Bid Service cluster. Bid Service: Stateless, horizontally scaled, in-memory feature cache. Latency budget: 20ms network, 30ms bid decision, 50ms margin. Components: 1) Feature Store: Pre-computed user/context features in Redis, updated async from data pipeline. 2) ML Model: Lightweight prediction model loaded in memory, predicts CTR/conversion probability. 3) Bid Calculator: Applies business rules, budget constraints, pacing to model output. 4) Budget Tracker: Real-time spend tracking in Redis (atomic increments), pacing algorithm limits bid rate. 5) Response Builder: Formats bid response, signs for verification. Scaling for 100K QPS: 50 bid service instances (2K QPS each), Redis cluster for features, horizontal scaling for peaks. Winner notification: Async webhook receiver, updates conversion tracking. Monitoring: Latency percentiles, win rate, spend rate, budget pacing accuracy. Optimization: Minimal allocations in hot path, connection pooling, pre-warmed feature cache. Fallback: No-bid response on timeout or error.",
      "explanation": "Extreme latency constraint requires in-memory processing. Pre-computed features avoid runtime lookups. Horizontal scaling handles high QPS.",
      "concept_ids": [
        "CONCEPT-032",
        "CONCEPT-040"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q157",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "You're designing a platform that will be white-labeled for multiple enterprise customers. Each customer wants customization: branding, feature toggles, custom fields. What's the best multi-tenancy approach?",
      "options": [
        "Shared infrastructure with tenant-aware customization layer, configuration per tenant, feature flags for optional features",
        "Separate deployment per customer",
        "Fork the codebase per customer",
        "No customization allowed"
      ],
      "correct_answer": "Shared infrastructure with tenant-aware customization layer, configuration per tenant, feature flags for optional features",
      "explanation": "Shared infrastructure reduces operational burden. Tenant-aware customization layer handles branding/fields. Feature flags enable per-tenant features. Single codebase is maintainable.",
      "concept_ids": [
        "CONCEPT-056",
        "CONCEPT-081"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q158",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a data replication strategy for a globally distributed database. Requirements: sub-50ms local reads, eventual consistency for non-critical data, strong consistency for financial data, conflict-free collaboration features.",
      "correct_answer": "Replication Strategy: Topology: Multi-master in 3 regions (US, EU, APAC). Data classification: 1) Financial data: Synchronous replication, single primary per record (assigned by region), cross-region writes route to primary. 2) User preferences: Async replication, LWW (last-writer-wins) conflict resolution. 3) Collaboration data: CRDTs for conflict-free merging (counters, sets, text). Local reads: All data replicated to local region, reads served locally (sub-50ms). Write path: Financial -> route to primary -> sync replicate -> ack. Preference -> local write -> async replicate. Collaboration -> local CRDT merge -> async sync. Conflict resolution: Financial: no conflicts (single writer), Preferences: timestamp-based LWW, Collaboration: CRDT rules (automatic merge). Lag monitoring: Replication lag alerts, fallback to primary read if lag exceeds threshold. Technology: CockroachDB for SQL with consistency options, or custom with Cassandra + CRDT layer. Consistency tuning: Per-query consistency level, not per-table.",
      "explanation": "Different consistency needs require different strategies. CRDTs solve collaboration conflicts. Routing to primary prevents financial data conflicts.",
      "concept_ids": [
        "CONCEPT-042",
        "CONCEPT-041"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q159",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a sustainable architecture for a cloud application. The company has committed to carbon neutrality. Which design choices most directly reduce environmental impact?",
      "options": [
        "Right-size resources for high utilization, schedule non-critical jobs for low-carbon periods, use serverless for variable workloads, prefer managed services",
        "Use the largest instances available",
        "Run 24/7 at peak capacity",
        "Deploy in multiple regions for redundancy only"
      ],
      "correct_answer": "Right-size resources for high utilization, schedule non-critical jobs for low-carbon periods, use serverless for variable workloads, prefer managed services",
      "explanation": "Higher utilization means less idle capacity. Scheduling for low-carbon grid periods reduces emissions. Serverless and managed services share resources efficiently.",
      "concept_ids": [
        "CONCEPT-071"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q160",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a GraphQL federation for a microservices architecture with 10 backend services. Requirements: unified schema for clients, service autonomy for teams, performance optimization, caching strategy.",
      "correct_answer": "GraphQL Federation Design: Architecture: Apollo Federation with gateway. Each service: Subgraph defining its portion of schema with @key directives for entity references. Gateway: Apollo Router or Gateway, composes subgraphs at startup, routes queries to appropriate subgraphs. Schema stitching: Entities define @key fields, other services use @extends to add fields to entities. Service autonomy: Each team owns their subgraph schema, deploy independently, gateway recomposes on restart. Performance: Query planning at gateway (minimize round trips), DataLoader pattern for N+1 prevention, batched subgraph calls. Caching: Response caching at gateway (Cache-Control hints from subgraphs), CDN for public queries, Redis for authenticated user caching, persisted queries for frequent operations. Monitoring: Per-subgraph latency, resolver timing, error rates, query complexity metrics. Tooling: Schema registry for version management, breaking change detection, schema linting. Tracing: Distributed tracing through gateway to subgraphs.",
      "explanation": "Federation enables service autonomy while presenting unified schema. Gateway handles composition and optimization. DataLoader prevents N+1 queries.",
      "concept_ids": [
        "CONCEPT-006",
        "CONCEPT-065"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q161",
      "question": "Complete the Circuit Breaker implementation in Python:\n\n```python\nclass CircuitBreaker:\n    def __init__(self, failure_threshold=5, reset_timeout=60):\n        self.failure_threshold = failure_threshold\n        self.reset_timeout = reset_timeout\n        self.failures = 0\n        self.state = 'CLOSED'\n        self.last_failure_time = None\n    \n    def call(self, func, *args, **kwargs):\n        # TODO: Implement circuit breaker logic\n        pass\n```\n\nImplement the `call` method that:\n1. Returns immediately with an exception if state is OPEN (unless timeout expired)\n2. Executes the function and resets failures on success\n3. Increments failures and potentially opens circuit on failure",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-024"
      ],
      "source": "azure_patterns",
      "expected_answer": "Implementation should check state, handle OPEN/HALF_OPEN/CLOSED transitions, track failures, and implement timeout-based recovery.",
      "rubric": {
        "full_credit": "Correct state machine implementation with all three states, proper timeout handling, and failure counting",
        "partial_credit": "Missing one state or incomplete timeout logic",
        "no_credit": "Fundamentally incorrect implementation"
      }
    },
    {
      "id": "Q162",
      "question": "Write a Kubernetes Deployment YAML for a microservice with the following requirements:\n- 3 replicas\n- Resource limits: 256Mi memory, 500m CPU\n- Liveness probe on /health endpoint\n- Readiness probe on /ready endpoint\n- Rolling update strategy with maxSurge=1, maxUnavailable=0",
      "skill": "implement",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-055"
      ],
      "source": "kubernetes_keps",
      "expected_answer": "Complete Deployment manifest with proper spec including replicas, resources, probes with httpGet, and strategy configuration."
    },
    {
      "id": "Q163",
      "question": "Given this Saga orchestrator pseudocode, identify and fix the bug:\n\n```python\nclass OrderSaga:\n    def execute(self):\n        try:\n            self.reserve_inventory()\n            self.charge_payment()\n            self.ship_order()\n        except PaymentError:\n            self.release_inventory()\n        except ShippingError:\n            self.refund_payment()\n```\n\nWhat compensating transactions are missing?",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-019"
      ],
      "source": "microservices_io",
      "expected_answer": "ShippingError handler should also release inventory. Need proper ordering: release_inventory() then refund_payment() for ShippingError."
    },
    {
      "id": "Q165",
      "question": "Implement a basic Retry with Exponential Backoff decorator in Python:\n\n```python\nimport time\nimport random\n\ndef retry_with_backoff(max_retries=3, base_delay=1.0, max_delay=60.0):\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            # TODO: Implement retry logic with exponential backoff and jitter\n            pass\n        return wrapper\n    return decorator\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-025"
      ],
      "source": "azure_patterns",
      "expected_answer": "Loop with try/except, calculate delay as min(base_delay * 2**attempt, max_delay), add random jitter, sleep between retries, raise after max_retries exceeded."
    },
    {
      "id": "Q166",
      "question": "Review this ADR and identify what's missing:\n\n```markdown\n# ADR-001: Use PostgreSQL for Order Service\n\n## Status\nAccepted\n\n## Context\nWe need a database for the order service.\n\n## Decision\nWe will use PostgreSQL.\n\n## Consequences\nWe need to set up PostgreSQL.\n```\n\nList at least 3 critical elements missing from this ADR.",
      "skill": "implement",
      "topic": "technical_debt",
      "difficulty": "easy",
      "format": "free_response",
      "concepts": [
        "CONCEPT-079"
      ],
      "source": "real_adrs",
      "expected_answer": "Missing: (1) Alternatives considered, (2) Rationale/justification for choice, (3) Specific consequences (both positive and negative), (4) Decision date, (5) Decision makers/stakeholders."
    },
    {
      "id": "Q167",
      "question": "Complete the Event Sourcing aggregate implementation:\n\n```python\nclass BankAccount:\n    def __init__(self, account_id):\n        self.account_id = account_id\n        self.balance = 0\n        self.events = []\n    \n    def apply_event(self, event):\n        # TODO: Apply event to update state\n        pass\n    \n    def deposit(self, amount):\n        # TODO: Create and apply event\n        pass\n    \n    def withdraw(self, amount):\n        # TODO: Create and apply event, handle insufficient funds\n        pass\n    \n    def replay_events(self, events):\n        # TODO: Rebuild state from event history\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-021"
      ],
      "source": "microservices_io",
      "expected_answer": "apply_event should pattern match on event type and update balance. deposit/withdraw create event dicts with type/amount, append to events list, call apply_event. replay_events iterates and applies each event."
    },
    {
      "id": "Q169",
      "question": "Implement a basic API Gateway rate limiter using Redis:\n\n```python\nimport redis\nimport time\n\nclass RateLimiter:\n    def __init__(self, redis_client, requests_per_minute=60):\n        self.redis = redis_client\n        self.limit = requests_per_minute\n    \n    def is_allowed(self, client_id: str) -> bool:\n        # TODO: Implement sliding window rate limiting\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-030",
        "CONCEPT-046"
      ],
      "source": "azure_patterns",
      "expected_answer": "Use Redis sorted set with timestamp scores. Add current timestamp, remove entries older than window, count remaining entries, compare against limit."
    },
    {
      "id": "Q171",
      "question": "Write a Kubernetes HorizontalPodAutoscaler manifest that:\n- Scales based on CPU (target 50%) AND memory (target 80%)\n- Minimum 2 replicas, maximum 20\n- Scale down stabilization window of 300 seconds",
      "skill": "implement",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-055",
        "CONCEPT-053"
      ],
      "source": "kubernetes_keps",
      "expected_answer": "HPA v2 manifest with metrics array containing both Resource type metrics, scaleTargetRef pointing to deployment, behavior.scaleDown.stabilizationWindowSeconds: 300."
    },
    {
      "id": "Q172",
      "question": "Complete the CQRS command handler:\n\n```python\nclass CreateOrderCommand:\n    def __init__(self, order_id, customer_id, items):\n        self.order_id = order_id\n        self.customer_id = customer_id\n        self.items = items\n\nclass CreateOrderHandler:\n    def __init__(self, event_store, event_publisher):\n        self.event_store = event_store\n        self.event_publisher = event_publisher\n    \n    def handle(self, command: CreateOrderCommand):\n        # TODO: Validate, create event, store, and publish\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-020"
      ],
      "source": "microservices_io",
      "expected_answer": "Validate command data, create OrderCreatedEvent with command data, persist to event_store, publish via event_publisher, return success/event_id."
    },
    {
      "id": "Q174",
      "question": "Implement the Strangler Fig pattern for migrating a legacy endpoint. Complete the proxy:\n\n```python\nclass StranglerProxy:\n    def __init__(self, legacy_url, new_service_url):\n        self.legacy_url = legacy_url\n        self.new_service_url = new_service_url\n        self.migrated_endpoints = set()\n    \n    async def route_request(self, method, path, body=None, headers=None):\n        # TODO: Route to new service if migrated, otherwise to legacy\n        pass\n    \n    def mark_migrated(self, path_pattern):\n        # TODO: Mark an endpoint as migrated\n        pass\n```",
      "skill": "implement",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-041",
        "CONCEPT-082"
      ],
      "source": "fowler",
      "expected_answer": "route_request checks if path matches any migrated_endpoints patterns, forwards to new_service_url if matched, legacy_url otherwise. mark_migrated adds pattern to set."
    },
    {
      "id": "Q176",
      "question": "Write a GitHub Actions workflow step that implements blue-green deployment verification:\n- Deploy to green environment\n- Run smoke tests against green\n- Switch traffic only if tests pass\n- Rollback if tests fail",
      "skill": "implement",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-058",
        "CONCEPT-059"
      ],
      "source": "real_adrs",
      "expected_answer": "Steps for: deploy to green, run smoke tests with continue-on-error, conditional step to switch traffic if tests passed, conditional rollback/cleanup if tests failed."
    },
    {
      "id": "Q177",
      "question": "What's the bug in this distributed lock implementation?\n\n```python\nimport redis\nimport uuid\n\nclass DistributedLock:\n    def __init__(self, redis_client, lock_name, ttl=30):\n        self.redis = redis_client\n        self.lock_name = lock_name\n        self.ttl = ttl\n        self.lock_id = str(uuid.uuid4())\n    \n    def acquire(self):\n        return self.redis.set(self.lock_name, self.lock_id, nx=True, ex=self.ttl)\n    \n    def release(self):\n        self.redis.delete(self.lock_name)\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-074"
      ],
      "source": "azure_patterns",
      "expected_answer": "Release doesn't verify lock ownership - could delete another process's lock. Should use Lua script or WATCH/MULTI to atomically check lock_id matches before deleting."
    },
    {
      "id": "Q178",
      "question": "Complete the Bulkhead pattern implementation using semaphores:\n\n```python\nimport asyncio\nfrom typing import Dict\n\nclass BulkheadManager:\n    def __init__(self):\n        self.bulkheads: Dict[str, asyncio.Semaphore] = {}\n    \n    def register_bulkhead(self, name: str, max_concurrent: int):\n        # TODO: Create semaphore for this bulkhead\n        pass\n    \n    async def execute(self, bulkhead_name: str, func, *args, **kwargs):\n        # TODO: Execute function within bulkhead limits\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-027"
      ],
      "source": "azure_patterns",
      "expected_answer": "register_bulkhead creates Semaphore(max_concurrent) in dict. execute uses 'async with self.bulkheads[bulkhead_name]' to acquire semaphore, then awaits func(*args, **kwargs)."
    },
    {
      "id": "Q180",
      "question": "Implement an Anti-Corruption Layer adapter:\n\n```python\n# Legacy system returns:\n# {'cust_no': '12345', 'cust_nm': 'John', 'addr_1': '123 Main St'}\n\n# New system expects:\n# {'customer_id': '12345', 'name': 'John', 'address': {'street': '123 Main St'}}\n\nclass CustomerACL:\n    def __init__(self, legacy_client):\n        self.legacy = legacy_client\n    \n    def get_customer(self, customer_id: str):\n        # TODO: Fetch from legacy and translate\n        pass\n    \n    def _translate_customer(self, legacy_data: dict) -> dict:\n        # TODO: Map legacy format to new format\n        pass\n```",
      "skill": "implement",
      "topic": "decomposition",
      "difficulty": "easy",
      "format": "free_response",
      "concepts": [
        "CONCEPT-042"
      ],
      "source": "azure_patterns",
      "expected_answer": "get_customer calls legacy.get(customer_id), passes result to _translate_customer. _translate_customer maps cust_no->customer_id, cust_nm->name, addr_1->address.street."
    },
    {
      "id": "Q181",
      "question": "Write the Kubernetes NetworkPolicy YAML to:\n- Allow ingress only from pods with label 'app: frontend'\n- Allow egress only to pods with label 'app: database' on port 5432\n- Deny all other traffic",
      "skill": "implement",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-004",
        "CONCEPT-055"
      ],
      "source": "kubernetes_keps",
      "expected_answer": "NetworkPolicy with podSelector, policyTypes [Ingress, Egress], ingress rule with from podSelector matching app:frontend, egress rule with to podSelector matching app:database and ports [5432]."
    },
    {
      "id": "Q183",
      "question": "Implement a simple outbox pattern for reliable event publishing:\n\n```python\nclass OutboxPublisher:\n    def __init__(self, db_session, message_broker):\n        self.db = db_session\n        self.broker = message_broker\n    \n    def save_with_event(self, entity, event):\n        # TODO: Save entity and event in same transaction\n        pass\n    \n    async def process_outbox(self):\n        # TODO: Publish pending events and mark as processed\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-022"
      ],
      "source": "microservices_io",
      "expected_answer": "save_with_event: begin transaction, save entity, insert event into outbox table with status='pending', commit. process_outbox: query pending events, for each publish to broker, update status='processed' (or delete), handle failures with retry."
    },
    {
      "id": "Q185",
      "question": "Implement feature flag evaluation with gradual rollout:\n\n```python\nimport hashlib\n\nclass FeatureFlagService:\n    def __init__(self):\n        self.flags = {}\n    \n    def set_flag(self, name: str, percentage: int, allowed_groups: list = None):\n        # TODO: Configure flag with percentage rollout and group targeting\n        pass\n    \n    def is_enabled(self, flag_name: str, user_id: str, user_groups: list = None) -> bool:\n        # TODO: Evaluate flag for user (consistent hashing for percentage)\n        pass\n```",
      "skill": "implement",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-059",
        "CONCEPT-080"
      ],
      "source": "real_adrs",
      "expected_answer": "set_flag stores config dict. is_enabled: check if user_groups intersect allowed_groups (if set), then hash(flag_name + user_id) % 100 < percentage for consistent rollout."
    },
    {
      "id": "Q186",
      "question": "Fix the race condition in this cache-aside implementation:\n\n```python\nclass CacheAside:\n    def __init__(self, cache, database):\n        self.cache = cache\n        self.db = database\n    \n    async def get(self, key):\n        value = await self.cache.get(key)\n        if value is None:\n            value = await self.db.get(key)\n            await self.cache.set(key, value)\n        return value\n    \n    async def update(self, key, value):\n        await self.db.update(key, value)\n        await self.cache.delete(key)\n```\n\nWhat's the race condition and how would you fix it?",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-074"
      ],
      "source": "azure_patterns",
      "expected_answer": "Race: Thread A reads stale from DB, Thread B updates DB and deletes cache, Thread A writes stale value to cache. Fix: Use cache.delete() before db.update(), or use distributed locking, or add TTL to cached values."
    },
    {
      "id": "Q187",
      "question": "Write a Dockerfile that follows security best practices for a Node.js application:\n- Non-root user\n- Multi-stage build\n- Minimal base image\n- No unnecessary packages",
      "skill": "implement",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-004",
        "CONCEPT-056"
      ],
      "source": "aws_wellarchitected",
      "expected_answer": "Multi-stage: builder stage with node:alpine, install deps, build; production stage with node:alpine, create non-root user, copy only built artifacts, USER directive, minimal CMD."
    },
    {
      "id": "Q189",
      "question": "Implement a simple service registry with health checking:\n\n```python\nimport asyncio\nfrom typing import Dict, List\nfrom dataclasses import dataclass\n\n@dataclass\nclass ServiceInstance:\n    id: str\n    host: str\n    port: int\n    healthy: bool = True\n\nclass ServiceRegistry:\n    def __init__(self, health_check_interval=30):\n        self.services: Dict[str, List[ServiceInstance]] = {}\n        self.interval = health_check_interval\n    \n    def register(self, service_name: str, instance: ServiceInstance):\n        # TODO: Register instance\n        pass\n    \n    def deregister(self, service_name: str, instance_id: str):\n        # TODO: Remove instance\n        pass\n    \n    def get_healthy_instances(self, service_name: str) -> List[ServiceInstance]:\n        # TODO: Return only healthy instances\n        pass\n    \n    async def health_check_loop(self):\n        # TODO: Periodically check all instances\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-063",
        "CONCEPT-064"
      ],
      "source": "microservices_io",
      "expected_answer": "register adds to dict list. deregister filters by id. get_healthy_instances filters where healthy=True. health_check_loop: while True, iterate all instances, HTTP GET /health, update healthy flag, await asyncio.sleep(interval)."
    },
    {
      "id": "Q191",
      "question": "Complete the BFF (Backend for Frontend) implementation:\n\n```python\nclass MobileBFF:\n    def __init__(self, user_service, order_service, product_service):\n        self.users = user_service\n        self.orders = order_service\n        self.products = product_service\n    \n    async def get_home_screen_data(self, user_id: str) -> dict:\n        # TODO: Aggregate data optimized for mobile home screen\n        # Should include: user profile summary, recent orders (last 3),\n        # recommended products (limit 5)\n        # Optimize for minimal payload size\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-047"
      ],
      "source": "microservices_io",
      "expected_answer": "Use asyncio.gather to fetch user, orders, products in parallel. Transform responses to include only needed fields (name not full profile, order summary not details, product thumbnails not full images). Return combined dict."
    },
    {
      "id": "Q192",
      "question": "Implement idempotency key handling for a payment API:\n\n```python\nclass PaymentService:\n    def __init__(self, db, payment_gateway):\n        self.db = db\n        self.gateway = payment_gateway\n    \n    async def process_payment(self, idempotency_key: str, amount: float, \n                              customer_id: str) -> dict:\n        # TODO: Implement idempotent payment processing\n        # - Check if request was already processed\n        # - If yes, return cached result\n        # - If no, process and store result\n        # - Handle in-progress requests\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-074"
      ],
      "source": "real_adrs",
      "expected_answer": "Check db for idempotency_key. If exists with status='completed', return stored result. If status='in_progress', return 409 or wait. If not exists, insert with status='in_progress', call gateway, update with result and status='completed', return result. Use transaction/lock."
    },
    {
      "id": "Q194",
      "question": "Implement a simple message deduplication handler:\n\n```python\nfrom datetime import datetime, timedelta\n\nclass MessageDeduplicator:\n    def __init__(self, redis_client, dedup_window_minutes=60):\n        self.redis = redis_client\n        self.window = dedup_window_minutes\n    \n    async def is_duplicate(self, message_id: str) -> bool:\n        # TODO: Check if message was seen within dedup window\n        pass\n    \n    async def mark_processed(self, message_id: str):\n        # TODO: Record message as processed\n        pass\n    \n    async def process_if_new(self, message_id: str, handler_func):\n        # TODO: Only process if not duplicate\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "easy",
      "format": "free_response",
      "concepts": [
        "CONCEPT-028"
      ],
      "source": "microservices_io",
      "expected_answer": "is_duplicate: return await redis.exists(f'dedup:{message_id}'). mark_processed: await redis.setex(f'dedup:{message_id}', window*60, '1'). process_if_new: if not await is_duplicate, call handler_func, then mark_processed."
    },
    {
      "id": "Q195",
      "question": "Fix the observability gap in this async task processor:\n\n```python\nasync def process_task(task):\n    result = await heavy_computation(task.data)\n    await save_result(task.id, result)\n    return result\n\nasync def worker():\n    while True:\n        task = await queue.get()\n        try:\n            await process_task(task)\n        except Exception:\n            pass\n        finally:\n            queue.task_done()\n```\n\nAdd proper logging, metrics, and tracing.",
      "skill": "implement",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-065",
        "CONCEPT-066",
        "CONCEPT-068"
      ],
      "source": "microservices_io",
      "expected_answer": "Add: structured logging with task_id/status, try/except with logger.exception(), metrics for tasks_processed/failed/duration histogram, span creation with task context, propagate trace_id if present in task."
    },
    {
      "id": "Q196",
      "question": "Implement a configuration-driven feature toggle system:\n\n```yaml\n# config.yaml\nfeatures:\n  new_checkout:\n    enabled: true\n    rollout_percentage: 25\n    whitelist_users: [\"user123\", \"user456\"]\n    blacklist_users: []\n```\n\n```python\nclass FeatureConfig:\n    def __init__(self, config_path: str):\n        # TODO: Load and parse config\n        pass\n    \n    def is_feature_enabled(self, feature_name: str, user_id: str = None) -> bool:\n        # TODO: Evaluate feature for user\n        pass\n```",
      "skill": "implement",
      "topic": "cloud_deployment",
      "difficulty": "easy",
      "format": "free_response",
      "concepts": [
        "CONCEPT-059"
      ],
      "source": "fowler",
      "expected_answer": "Load YAML in __init__. is_feature_enabled: get feature config, check enabled flag, if user_id in blacklist return False, if in whitelist return True, else hash(user_id) % 100 < rollout_percentage."
    },
    {
      "id": "Q198",
      "question": "Implement a circuit breaker state machine with proper transitions:\n\n```python\nfrom enum import Enum\nfrom datetime import datetime, timedelta\n\nclass State(Enum):\n    CLOSED = 'closed'\n    OPEN = 'open'\n    HALF_OPEN = 'half_open'\n\nclass CircuitBreakerStateMachine:\n    def __init__(self, failure_threshold=5, success_threshold=3, \n                 timeout_seconds=60):\n        self.failure_threshold = failure_threshold\n        self.success_threshold = success_threshold\n        self.timeout = timedelta(seconds=timeout_seconds)\n        # TODO: Initialize state variables\n    \n    def record_success(self):\n        # TODO: Handle success in current state\n        pass\n    \n    def record_failure(self):\n        # TODO: Handle failure in current state\n        pass\n    \n    def can_execute(self) -> bool:\n        # TODO: Check if request should be allowed\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-024"
      ],
      "source": "azure_patterns",
      "expected_answer": "Track state, failure_count, success_count, last_failure_time. record_success: in HALF_OPEN increment success_count, if >= threshold go CLOSED and reset. record_failure: increment failure_count, if CLOSED and >= threshold go OPEN with timestamp, if HALF_OPEN go OPEN. can_execute: CLOSED=true, OPEN=check if timeout expired then go HALF_OPEN and return true else false, HALF_OPEN=true."
    },
    {
      "id": "Q199",
      "question": "Complete this ADR template for choosing between REST and gRPC:\n\n```markdown\n# ADR-XXX: API Protocol for Inter-Service Communication\n\n## Status\n[TODO]\n\n## Context\nWe need to choose an API protocol for communication between our microservices.\nCurrent services: Order, Inventory, Payment, Notification\nRequirements: Low latency (<10ms p99), type safety, streaming for notifications\n\n## Decision\n[TODO - Choose and justify]\n\n## Consequences\n[TODO - List positive and negative consequences]\n```",
      "skill": "implement",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "free_response",
      "concepts": [
        "CONCEPT-079",
        "CONCEPT-029"
      ],
      "source": "real_adrs",
      "expected_answer": "Status: Proposed/Accepted. Decision: gRPC for internal services due to latency requirements, type safety via protobuf, native streaming support. Consequences: (+) Performance, type safety, streaming; (-) Debugging harder, need protobuf tooling, team learning curve, REST gateway needed for external clients."
    },
    {
      "id": "Q200",
      "question": "Implement a basic leader election mechanism using Redis:\n\n```python\nimport redis\nimport asyncio\nfrom typing import Optional, Callable\n\nclass LeaderElection:\n    def __init__(self, redis_client, election_key: str, \n                 instance_id: str, ttl_seconds: int = 30):\n        self.redis = redis_client\n        self.key = election_key\n        self.instance_id = instance_id\n        self.ttl = ttl_seconds\n        self.is_leader = False\n    \n    async def try_become_leader(self) -> bool:\n        # TODO: Attempt to acquire leadership\n        pass\n    \n    async def renew_leadership(self) -> bool:\n        # TODO: Extend TTL if still leader\n        pass\n    \n    async def resign(self):\n        # TODO: Give up leadership\n        pass\n    \n    async def leader_loop(self, on_leader: Callable, on_follower: Callable):\n        # TODO: Continuously try to become/stay leader\n        pass\n```",
      "skill": "implement",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "concepts": [
        "CONCEPT-074"
      ],
      "source": "azure_patterns",
      "expected_answer": "try_become_leader: SET key instance_id NX EX ttl, return success. renew_leadership: check if current value == instance_id, if yes EXPIRE/SET with new ttl, else return false. resign: check ownership then DELETE. leader_loop: while True, if is_leader try renew else try become, call appropriate callback, sleep ttl/3."
    }
  ]
}