{
  "batch": "design",
  "count": 50,
  "questions": [
    {
      "id": "Q111",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a payment processing system for an e-commerce platform. Requirements: 99.99% availability, transactions must not be lost, exactly-once payment processing, support 10,000 TPS at peak. The payment gateway is external and occasionally slow (up to 30s). Describe your architecture including patterns used.",
      "correct_answer": "Architecture: 1) API layer with rate limiting and circuit breaker for payment gateway. 2) Transactional Outbox pattern to atomically record payment intent and queue message. 3) Idempotent payment processor with idempotency keys for exactly-once semantics. 4) Saga with orchestration for multi-step payment flow (reserve funds, process payment, update order). 5) Bulkhead isolation for payment types. 6) Event sourcing for payment audit trail. 7) Queue-based load leveling with competing consumers to handle 10K TPS. 8) Multi-AZ deployment for 99.99%. 9) Timeout + async completion for slow gateway calls with webhook callbacks. Key patterns: Circuit Breaker, Transactional Outbox, Saga, Bulkhead, Idempotent Consumer.",
      "explanation": "Payment systems require careful handling of failures, exactly-once semantics, and high availability. This design addresses each requirement with appropriate patterns.",
      "concept_ids": [
        "CONCEPT-001",
        "CONCEPT-002",
        "CONCEPT-009",
        "CONCEPT-019",
        "CONCEPT-024"
      ],
      "sources": [
        "microservices.io",
        "Azure Patterns"
      ]
    },
    {
      "id": "Q112",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "You're tasked with decomposing a 5-year-old monolithic CRM system with 300K lines of code into microservices. The system handles: customer management, sales pipeline, support tickets, reporting, and email campaigns. Design a migration strategy and initial service boundaries.",
      "correct_answer": "Strategy: Strangler Fig pattern for incremental migration. Phase 1: Deploy facade/API Gateway in front of monolith. Phase 2: Extract low-risk, high-value services first. Suggested boundaries based on business capabilities: 1) Customer Service (core customer data, CRUD), 2) Sales Service (pipeline, opportunities, forecasting), 3) Support Service (tickets, SLAs, escalation), 4) Campaign Service (email marketing, automation), 5) Reporting Service (CQRS read models, aggregations). Anti-Corruption Layer between new services and monolith. Database per Service gradually, starting with new features. Shared data initially via API calls or event-driven updates. Migration order: Campaign (low coupling) -> Support -> Sales -> Customer (highest risk, most dependencies) -> Reporting (aggregate view).",
      "explanation": "Strangler Fig with business capability decomposition reduces risk. Start with loosely coupled services, build team experience, then tackle tightly coupled core.",
      "concept_ids": [
        "CONCEPT-008",
        "CONCEPT-016",
        "CONCEPT-015",
        "CONCEPT-005"
      ],
      "sources": [
        "Martin Fowler",
        "microservices.io"
      ]
    },
    {
      "id": "Q113",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a disaster recovery strategy for a critical financial application. Requirements: RTO of 4 hours, RPO of 15 minutes, budget constraints prevent active-active in multiple regions. Which approach best fits?",
      "options": [
        "Warm standby in secondary region with continuous replication and automated failover",
        "Daily backups to another region",
        "Active-active multi-region deployment",
        "Cold standby with weekly backups"
      ],
      "correct_answer": "Warm standby in secondary region with continuous replication and automated failover",
      "explanation": "Warm standby meets the 4-hour RTO (quick spinup) and 15-minute RPO (continuous replication) without the cost of active-active. Cold standby is too slow for 4-hour RTO.",
      "concept_ids": [
        "CONCEPT-033",
        "CONCEPT-034"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q114",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design an order fulfillment system that coordinates: inventory reservation, payment processing, warehouse picking, and shipping. Each step involves different services. Payment and inventory can fail. Design the transaction management approach.",
      "correct_answer": "Use Saga pattern with orchestration. Orchestrator: Order Fulfillment Saga Coordinator. Steps: 1) Reserve Inventory (compensate: release inventory), 2) Process Payment (compensate: refund), 3) Create Pick List (compensate: cancel pick), 4) Initiate Shipping (compensate: cancel shipment). Implementation: Saga orchestrator as separate service storing saga state. Each step uses Transactional Outbox for reliable messaging. Idempotency keys for all operations. Timeout handling with status polling for long operations. Saga state machine: PENDING -> INVENTORY_RESERVED -> PAYMENT_PROCESSED -> PICKING -> SHIPPED. On failure at any step, execute compensating transactions in reverse order. Consider: Inventory reservation timeout, payment webhook callbacks, manual intervention for stuck sagas.",
      "explanation": "Orchestrated saga provides clear flow visibility for order fulfillment. Compensating transactions handle failures at each step.",
      "concept_ids": [
        "CONCEPT-002",
        "CONCEPT-079",
        "CONCEPT-080",
        "CONCEPT-019"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q115",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a logging and monitoring strategy for 50 microservices. Requirements: centralized view, request tracing across services, alerting on SLO violations, 30-day log retention. Which stack best fits?",
      "options": [
        "ELK for logs, Prometheus/Grafana for metrics, Jaeger for tracing, with correlation IDs",
        "Console.log in each service",
        "Centralized relational database for all logs",
        "Separate logging per service without aggregation"
      ],
      "correct_answer": "ELK for logs, Prometheus/Grafana for metrics, Jaeger for tracing, with correlation IDs",
      "explanation": "This stack provides the three pillars of observability. ELK handles log aggregation, Prometheus/Grafana enables SLO alerting, Jaeger provides distributed tracing. Correlation IDs tie them together.",
      "concept_ids": [
        "CONCEPT-020",
        "CONCEPT-021",
        "CONCEPT-085"
      ],
      "sources": [
        "microservices.io",
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q116",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design API versioning strategy for a public API with 500 enterprise customers. Requirements: 6-month deprecation window, clear migration path, support parallel versions, minimize customer disruption. Include versioning approach and deprecation process.",
      "correct_answer": "Versioning approach: URL path versioning (/v1/, /v2/) for clarity and routing simplicity. Process: 1) New version development in parallel with current version. 2) Announce deprecation with 6-month countdown in API responses (Sunset header, deprecation warnings). 3) Provide migration guides and SDK updates. 4) Analytics to track version usage per customer. 5) Direct outreach to customers still on deprecated versions. 6) Grace period extensions for strategic customers. 7) Final deprecation with clear cutoff date. Technical: API Gateway routes by version, shared backend services where compatible, version-specific adapters where needed. Documentation: Changelog, migration guide, version comparison, sunset timeline. Communication: Email announcements, developer portal notices, API response warnings.",
      "explanation": "URL versioning is most explicit for enterprise customers. The deprecation process balances business needs with technical evolution.",
      "concept_ids": [
        "CONCEPT-057",
        "CONCEPT-068"
      ],
      "sources": [
        "ADRs",
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q117",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a caching strategy for a product catalog service. Requirements: 1M products, 100K reads/minute, updates every 5 minutes from suppliers, 99.9% cache hit rate target. Products have images (10MB each) and metadata (1KB). Which approach?",
      "options": [
        "Redis for metadata cache-aside, CDN for images, async cache invalidation on updates",
        "Cache everything in Redis including images",
        "No caching, scale the database",
        "Browser caching only"
      ],
      "correct_answer": "Redis for metadata cache-aside, CDN for images, async cache invalidation on updates",
      "explanation": "Separate concerns: Redis efficiently caches small metadata, CDN handles large images at edge. Cache-aside with async invalidation handles the 5-minute update cycle appropriately.",
      "concept_ids": [
        "CONCEPT-038"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q118",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a zero-trust security architecture for a microservices platform. Requirements: service-to-service authentication, encryption in transit, fine-grained authorization, audit logging, no network-perimeter-based trust.",
      "correct_answer": "Architecture components: 1) Service Mesh (Istio) for automatic mTLS between all services, 2) SPIFFE/SPIRE for service identity and certificate management, 3) JWT tokens with short expiry for service-to-service auth, 4) OPA (Open Policy Agent) for fine-grained authorization policies, 5) Centralized identity provider (Auth0/Keycloak) for user auth, 6) API Gateway for external traffic with authentication, 7) Network policies limiting pod-to-pod communication, 8) Audit logging of all authentication/authorization decisions. Key principles: Never trust network location, verify every request, least privilege by default, assume breach mentality, encrypt everything. Implementation: All services require valid mTLS certificates, authorization checked at each service, all access decisions logged to SIEM.",
      "explanation": "Zero-trust requires multiple layers: identity verification, encryption, authorization, and comprehensive logging without trusting network boundaries.",
      "concept_ids": [
        "CONCEPT-030",
        "CONCEPT-031",
        "CONCEPT-046",
        "CONCEPT-059"
      ],
      "sources": [
        "AWS Well-Architected",
        "microservices.io"
      ]
    },
    {
      "id": "Q119",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "You're designing a multi-tenant SaaS platform. Large enterprise tenants need dedicated resources; small tenants can share resources. Which deployment model?",
      "options": [
        "Hybrid: dedicated infrastructure for enterprise tenants, shared pool for small tenants with tenant-aware routing",
        "Single shared infrastructure for all tenants",
        "Dedicated infrastructure for every tenant regardless of size",
        "Separate codebase per tenant"
      ],
      "correct_answer": "Hybrid: dedicated infrastructure for enterprise tenants, shared pool for small tenants with tenant-aware routing",
      "explanation": "Hybrid balances cost (shared for small tenants) with enterprise requirements (dedicated for large tenants). Routing layer directs traffic appropriately.",
      "concept_ids": [
        "CONCEPT-009"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q120",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design an event-driven architecture for a ride-sharing application. Key events: ride requested, driver assigned, ride started, ride completed, payment processed. Ensure reliability and handle driver no-shows.",
      "correct_answer": "Event-driven design: Event Store (Kafka) as source of truth. Domain Events: RideRequested, DriverAssigned, RideStarted, RideCompleted, PaymentProcessed, DriverNoShow. Services: 1) Ride Service (publishes ride events), 2) Matching Service (consumes RideRequested, publishes DriverAssigned), 3) Driver Service (consumes assignments, publishes location updates), 4) Payment Service (consumes RideCompleted, publishes PaymentProcessed), 5) Notification Service (consumes all events for user notifications). No-show handling: Matching Service sets timeout after DriverAssigned. If RideStarted not received within timeout, publish DriverNoShow, trigger reassignment, update driver reliability score. Reliability: Event Sourcing for ride state, Transactional Outbox for publishing, Idempotent consumers, Dead letter queue for failed processing. CQRS: Read models for real-time dashboards (driver locations, pending rides).",
      "explanation": "Event-driven fits ride-sharing's reactive nature. Timeout-based no-show detection with event-driven reassignment provides reliable handling.",
      "concept_ids": [
        "CONCEPT-004",
        "CONCEPT-023",
        "CONCEPT-062",
        "CONCEPT-003"
      ],
      "sources": [
        "microservices.io",
        "Martin Fowler"
      ]
    },
    {
      "id": "Q121",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a CI/CD pipeline for a microservices team. Requirements: 10 services in a monorepo, independent deployments, automated testing, deployment to Kubernetes. Which approach?",
      "options": [
        "Selective builds based on changed paths, service-specific pipelines, GitOps with ArgoCD for deployment",
        "Single pipeline that builds and deploys everything on every change",
        "Manual deployments after testing",
        "Branch per service with merge to main for deployment"
      ],
      "correct_answer": "Selective builds based on changed paths, service-specific pipelines, GitOps with ArgoCD for deployment",
      "explanation": "Path-based selective builds handle monorepo efficiency. Service-specific pipelines enable independent deployments. ArgoCD provides GitOps-based Kubernetes deployment.",
      "concept_ids": [
        "CONCEPT-052",
        "CONCEPT-053",
        "CONCEPT-036"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q122",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a rate limiting strategy for a multi-tier API. Requirements: 1000 req/min for free tier, 10000 req/min for paid tier, 100000 req/min for enterprise. Must prevent abuse while allowing legitimate bursts.",
      "correct_answer": "Implementation: Token Bucket algorithm with per-tier configuration. Free: 1000 tokens/minute, burst capacity 100. Paid: 10000 tokens/minute, burst capacity 1000. Enterprise: 100000 tokens/minute, burst capacity 10000. Architecture: 1) Distributed rate limiter using Redis for token counts, 2) API Gateway enforces limits before request processing, 3) Rate limit headers in responses (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset), 4) HTTP 429 response with Retry-After header when exceeded, 5) Per-API-key tracking for multi-key enterprise accounts, 6) Sliding window for fair distribution, 7) Graceful degradation: read-only access when write limits exceeded, 8) Alerting when customers approach limits for upsell opportunity. Anti-abuse: IP-based limits as backstop, anomaly detection for unusual patterns.",
      "explanation": "Token bucket allows bursts while enforcing average rates. Distributed implementation with Redis ensures consistency across instances.",
      "concept_ids": [
        "CONCEPT-011"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q123",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a notification service for a social media platform. Requirements: 1M+ users, real-time push, email digest, user preferences, guaranteed delivery for important notifications. Which architecture?",
      "options": [
        "Event-driven with priority queues, notification service subscribing to user events, preference-based routing, dead letter queue for retry",
        "Direct database inserts from all services",
        "Synchronous API calls for each notification",
        "Batch processing only"
      ],
      "correct_answer": "Event-driven with priority queues, notification service subscribing to user events, preference-based routing, dead letter queue for retry",
      "explanation": "Event-driven decouples notification from source services. Priority queues ensure important notifications aren't delayed. Preference routing respects user choices.",
      "concept_ids": [
        "CONCEPT-023",
        "CONCEPT-012"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q124",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a data mesh architecture for a large enterprise with multiple business domains: Sales, Marketing, Finance, Operations. Each domain should own their data products. Include governance and interoperability considerations.",
      "correct_answer": "Data Mesh Design: Domain-oriented ownership: Each domain (Sales, Marketing, Finance, Ops) owns data products as first-class citizens. Infrastructure: 1) Self-serve data platform providing compute, storage, and catalog capabilities, 2) Federated computational governance with automated policy enforcement, 3) Standardized interfaces (APIs, event streams) for data product consumption. Data Products per domain: Sales (leads, opportunities, forecasts), Marketing (campaigns, attribution, segments), Finance (revenue, costs, budgets), Ops (inventory, fulfillment, logistics). Interoperability: 1) Standard metadata schemas across domains, 2) Data contracts with SLOs, 3) Cross-domain access via governed APIs, 4) Central catalog for discovery. Governance: Automated quality checks, data lineage tracking, access policies, retention compliance. Implementation: Start with one domain, prove model, expand. Anti-pattern to avoid: central data team owning all data.",
      "explanation": "Data mesh applies domain-driven design to data, with domains owning their data products. Self-serve platform and federated governance enable scaling.",
      "concept_ids": [
        "CONCEPT-017",
        "CONCEPT-018",
        "CONCEPT-065"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q125",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design auto-scaling for an e-commerce platform expecting 10x traffic during flash sales (known events) and 3x spikes during normal operation (unknown events). Which scaling configuration?",
      "options": [
        "Scheduled scaling for known events, target tracking for normal operation, with pre-warming for flash sales",
        "Manual scaling before flash sales",
        "Only reactive auto-scaling based on CPU",
        "Fixed capacity for peak load"
      ],
      "correct_answer": "Scheduled scaling for known events, target tracking for normal operation, with pre-warming for flash sales",
      "explanation": "Scheduled scaling pre-provisions for known 10x events. Target tracking handles unknown 3x spikes reactively. Pre-warming ensures instances are ready for traffic.",
      "concept_ids": [
        "CONCEPT-040"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q126",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a search service for a marketplace with 50M products, real-time inventory updates, personalized results, and fuzzy matching. Include indexing strategy and consistency approach.",
      "correct_answer": "Architecture: 1) Elasticsearch cluster for search indexing with product data, 2) Event-driven updates via Kafka for real-time inventory changes, 3) Product Service as source of truth for product data, 4) CDC (Change Data Capture) for database-to-Kafka sync, 5) Personalization Service enriching queries with user preferences. Indexing Strategy: Product index with denormalized data (title, description, categories, price, availability), Inventory as near-real-time field updated via lightweight events, User preference index for personalization signals. Consistency: Eventual consistency acceptable for search (stale inventory OK for few seconds), Critical inventory checks at checkout with source of truth. Fuzzy matching: Elasticsearch n-gram tokenizer, phonetic analysis, synonym expansion, spell correction. Query flow: API Gateway -> Query Parser -> Personalization enrichment -> Elasticsearch -> Results ranking -> Response. Scaling: Shard by category, read replicas for query load.",
      "explanation": "Elasticsearch handles search requirements. Event-driven updates provide near-real-time inventory. Eventual consistency is acceptable for search with checkout validation.",
      "concept_ids": [
        "CONCEPT-042",
        "CONCEPT-003"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q127",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design an authentication system for a platform with web app, mobile app, and third-party API access. Requirements: SSO, MFA support, token refresh, and API keys for developers.",
      "options": [
        "OAuth 2.0 with OIDC for user auth, separate API key management for developers, JWT with refresh tokens, MFA via TOTP",
        "Session cookies for everything",
        "API keys for all access types",
        "Basic authentication with long-lived tokens"
      ],
      "correct_answer": "OAuth 2.0 with OIDC for user auth, separate API key management for developers, JWT with refresh tokens, MFA via TOTP",
      "explanation": "OAuth 2.0/OIDC is the standard for user authentication with SSO. API keys suit developer/machine access. JWTs enable stateless verification with refresh for longevity.",
      "concept_ids": [
        "CONCEPT-058",
        "CONCEPT-059"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q128",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design service boundaries for a banking application covering: accounts, transactions, loans, cards, and fraud detection. Consider data ownership, transaction boundaries, and regulatory requirements.",
      "correct_answer": "Service Boundaries: 1) Account Service: customer accounts, balances, account lifecycle (data owner: account data), 2) Transaction Service: deposits, withdrawals, transfers (orchestrates cross-account), 3) Loan Service: loan applications, disbursements, repayments (separate lifecycle), 4) Card Service: card issuance, limits, PIN management (security isolation), 5) Fraud Service: real-time fraud scoring, pattern detection (receives events from all). Transaction boundaries: Cross-account transfers use Saga (reserve source -> credit destination -> confirm source). Strong consistency within service, eventual between services. Regulatory: Audit logging in all services, immutable transaction history (event sourcing consideration for Transaction Service), data residency per account region. Data ownership: Each service owns its domain data. Shared reference data (customer profile) accessed via API with caching. Event-driven: All services publish domain events for fraud detection consumption.",
      "explanation": "Banking requires careful service boundaries around data ownership and regulatory needs. Strong consistency within financial transactions, saga for cross-service.",
      "concept_ids": [
        "CONCEPT-016",
        "CONCEPT-005",
        "CONCEPT-002"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q129",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a secrets management strategy for 100 microservices on Kubernetes. Requirements: automatic rotation, audit trail, development/production isolation, emergency revocation.",
      "options": [
        "HashiCorp Vault with Kubernetes auth, dynamic secrets, audit logging, and namespace isolation",
        "Kubernetes Secrets with GitOps",
        "Environment variables in deployment manifests",
        "Hardcoded in application config"
      ],
      "correct_answer": "HashiCorp Vault with Kubernetes auth, dynamic secrets, audit logging, and namespace isolation",
      "explanation": "Vault provides dynamic secrets with automatic rotation, comprehensive audit logging, and fine-grained access control. Kubernetes auth integrates with pod identity.",
      "concept_ids": [
        "CONCEPT-030",
        "CONCEPT-081"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q130",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a real-time analytics dashboard for an IoT platform. Requirements: 100K devices sending data every second, 5-second latency for dashboard updates, historical queries for last 30 days, anomaly alerts.",
      "correct_answer": "Architecture: Lambda architecture combining real-time and batch. Ingestion: Kafka for device data ingestion (partitioned by device ID). Stream processing: Apache Flink/Spark Streaming for real-time aggregations (5-second windows). Speed layer: Time-series database (InfluxDB/TimescaleDB) for recent hot data (7 days). Batch layer: Data lake (S3/HDFS) with Spark batch jobs for historical aggregations. Serving: Pre-computed dashboards from both layers, merged at query time. Anomaly detection: Flink stream processor with ML model, alerts via Kafka to Notification Service. Dashboard: WebSocket connection for real-time updates, REST API for historical queries. Scaling: Kafka partitions = device count / 1000, Flink parallelism based on throughput, time-series DB sharded by time. Data model: device_id, timestamp, metrics map, metadata. Retention: Hot (7 days, high resolution) -> Warm (30 days, downsampled) -> Cold (archive, highly compressed).",
      "explanation": "Lambda architecture handles both real-time and historical requirements. Time-series DB optimized for IoT data patterns. Stream processing enables low-latency dashboards.",
      "concept_ids": [
        "CONCEPT-023"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q131",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a data backup strategy for a globally distributed application with databases in 3 regions. Requirements: cross-region recovery, 1-hour RPO, 4-hour RTO, encryption at rest.",
      "options": [
        "Continuous replication to central backup region, point-in-time recovery enabled, encrypted snapshots, tested restore procedures",
        "Daily backups to same region",
        "Manual exports weekly",
        "No backups, rely on replication"
      ],
      "correct_answer": "Continuous replication to central backup region, point-in-time recovery enabled, encrypted snapshots, tested restore procedures",
      "explanation": "Continuous replication meets 1-hour RPO. Central backup enables cross-region recovery. Point-in-time recovery provides flexibility. Tested procedures ensure 4-hour RTO.",
      "concept_ids": [
        "CONCEPT-033",
        "CONCEPT-034"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q132",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a content moderation system for a user-generated content platform. Requirements: text, image, and video moderation; human review workflow; appeals process; audit trail; sub-100ms for text, 24h SLA for video.",
      "correct_answer": "Architecture: Event-driven with content type routing. Services: 1) Content Ingestion Service: receives uploads, publishes ContentSubmitted events, 2) Text Moderation Service: ML-based text analysis, sync for sub-100ms requirement, 3) Image Moderation Service: async with ML + hash-based matching, 4) Video Moderation Service: queue-based with 24h SLA, can use external provider, 5) Human Review Service: workflow for escalated content, 6) Appeals Service: resubmission workflow with different reviewers, 7) Audit Service: immutable event log of all decisions. Flow: Content submitted -> type-specific moderation -> ML scoring -> auto-approve/reject/escalate based on confidence. Human review for medium confidence and appeals. Escalation queue with priority. Audit: Event sourcing for complete decision history, who decided, when, why. Metrics: Moderation latency, false positive/negative rates, reviewer queue depth. Scaling: Text sync in-memory, Image/Video via queue with competing consumers.",
      "explanation": "Different content types have different latency requirements. ML for automation, human review for edge cases. Event sourcing provides audit trail.",
      "concept_ids": [
        "CONCEPT-012",
        "CONCEPT-004",
        "CONCEPT-051"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q133",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design message handling for an order system where processing must happen exactly once. Messages may be redelivered due to consumer failures. Which approach ensures correctness?",
      "options": [
        "Idempotent consumer with message ID tracking in the same transaction as business logic",
        "Rely on exactly-once delivery from message broker",
        "Process all messages regardless of duplicates",
        "Manual deduplication by operators"
      ],
      "correct_answer": "Idempotent consumer with message ID tracking in the same transaction as business logic",
      "explanation": "Most message systems provide at-least-once delivery. Idempotent consumers with transactional ID tracking ensure exactly-once processing semantics.",
      "concept_ids": [
        "CONCEPT-024"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q134",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a cost optimization strategy for a cloud-native startup spending $200K/month on AWS. They have production, staging, and development environments. Workloads are variable with peaks on weekdays.",
      "correct_answer": "Analysis and Strategy: 1) Environment scheduling: Shut down dev/staging outside business hours (save ~60% on non-prod), 2) Right-sizing: Analyze utilization, downsize over-provisioned instances (typical 20-30% savings), 3) Reserved capacity: Commit to 1-year Reserved Instances for steady-state production workloads (30-40% savings), 4) Spot instances: Use for fault-tolerant batch processing, CI/CD builds (60-90% savings), 5) Storage tiering: S3 lifecycle policies moving old data to Glacier (90% savings on archive), 6) Data transfer: Review cross-AZ/region traffic, consolidate where possible, 7) Right-size databases: RDS instance sizing, consider Aurora Serverless for variable loads, 8) Container optimization: Pod right-sizing, cluster autoscaling, Spot for non-critical pods. Implementation: Cost allocation tags, weekly cost review, per-team budgets, automated anomaly alerts. Expected outcome: 40-50% reduction ($80-100K/month savings).",
      "explanation": "Multi-pronged approach: scheduling for non-prod, reservations for stable prod, spot for batch, storage tiering, right-sizing everywhere.",
      "concept_ids": [
        "CONCEPT-038",
        "CONCEPT-039"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q135",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design an SLO framework for a microservices platform. You need to define SLOs, measure SLIs, and create error budgets. Which approach is most comprehensive?",
      "options": [
        "Define latency (p99) and availability SLOs per service, measure with distributed tracing, calculate error budget burn rate for release decisions",
        "Track only uptime percentage",
        "Use average latency for all measurements",
        "Measure only customer complaints"
      ],
      "correct_answer": "Define latency (p99) and availability SLOs per service, measure with distributed tracing, calculate error budget burn rate for release decisions",
      "explanation": "P99 latency captures tail experience. Per-service SLOs enable accountability. Error budgets tie reliability to deployment velocity.",
      "concept_ids": [
        "CONCEPT-021",
        "CONCEPT-020"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q136",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a multi-region active-active deployment for a global SaaS application. Requirements: local latency for users in each region, data sovereignty compliance, conflict resolution for concurrent updates, 99.99% global availability.",
      "correct_answer": "Architecture: Active-active deployment in 3 regions (US, EU, APAC). Traffic routing: Global load balancer (CloudFront/CloudFlare) routing to nearest region. Data strategy: Per-region primary for user data (data sovereignty), async replication with conflict resolution. Conflict resolution: Last-writer-wins with vector clocks for non-critical data, application-level merge for business-critical data, CRDTs where applicable. Database: Multi-region capable database (CockroachDB, Spanner, or managed solutions). Regional isolation: User data stays in region of registration. Cross-region: Shared reference data replicated async. Failure handling: Region failure -> traffic rerouted to next nearest region within seconds. Deployment: Independent deployments per region, feature flags for regional rollout. Monitoring: Global dashboard with per-region health, cross-region latency tracking. Availability math: 3 independent regions at 99.9% each = 99.9999% combined (if any region can serve any user).",
      "explanation": "Active-active requires careful conflict resolution. Data sovereignty addressed via regional data placement. Global LB provides failover and latency optimization.",
      "concept_ids": [
        "CONCEPT-033",
        "CONCEPT-042"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q137",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design data contracts between services in a microservices architecture. The Order Service needs Customer data from Customer Service. Which approach minimizes coupling while ensuring reliability?",
      "options": [
        "Customer Service publishes events with required data, Order Service maintains local cache updated via events",
        "Order Service directly queries Customer database",
        "Shared Customer library across all services",
        "Synchronous API call on every order"
      ],
      "correct_answer": "Customer Service publishes events with required data, Order Service maintains local cache updated via events",
      "explanation": "Event-driven data sharing minimizes runtime coupling. Local cache provides availability. Events define the contract. This is the Data Mesh/event-carried state transfer pattern.",
      "concept_ids": [
        "CONCEPT-005",
        "CONCEPT-062"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q138",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a Kubernetes deployment strategy for a critical service requiring zero-downtime deployments, rollback capability, and gradual traffic shifting. Include health checks and observability.",
      "correct_answer": "Deployment Strategy: Rolling update with canary promotion. Configuration: Deployment with: maxUnavailable: 0 (zero downtime), maxSurge: 25%, minReadySeconds: 60 (stability check), progressDeadlineSeconds: 600. Health checks: livenessProbe (restart if unhealthy), readinessProbe (traffic routing), startupProbe (slow-start apps). Canary: Use Argo Rollouts or Flagger for progressive delivery. Stages: 10% traffic -> metrics check -> 50% -> metrics check -> 100%. Rollback triggers: Error rate > 1%, P99 latency > 500ms, crash loop detected. Observability: Prometheus metrics scraping, custom metrics for business KPIs, Grafana dashboards per deployment, alerts on SLO breach. Rollback: Automatic on health check failure, manual via kubectl rollout undo. Testing: Pre-deployment integration tests, post-deployment smoke tests. Manifests: Separate ConfigMaps/Secrets with checksums for config-triggered rollouts.",
      "explanation": "Rolling updates with careful health checks enable zero-downtime. Canary with automated analysis catches issues before full rollout. Quick rollback minimizes impact.",
      "concept_ids": [
        "CONCEPT-055",
        "CONCEPT-022",
        "CONCEPT-025"
      ],
      "sources": [
        "ADRs",
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q139",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a chaos engineering program for a company new to the practice. They have 20 microservices in production with basic monitoring. What's the best starting approach?",
      "options": [
        "Start with GameDays in staging, simple failure injection (pod kill), graduate to production with small blast radius",
        "Immediately run chaos experiments in production at scale",
        "Only theoretical analysis without actual injection",
        "Chaos engineering only after all services are fully resilient"
      ],
      "correct_answer": "Start with GameDays in staging, simple failure injection (pod kill), graduate to production with small blast radius",
      "explanation": "Start small and safe: staging first, simple failures, build confidence and improve observability before production. Gradually increase scope and complexity.",
      "concept_ids": [
        "CONCEPT-037"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q140",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a recommendation engine for an e-commerce platform. Requirements: personalized product recommendations, new user cold start handling, real-time updates based on browsing, batch model training, A/B testing capability.",
      "correct_answer": "Architecture: 1) Data Collection Layer: Click stream collector via Kafka, user events aggregator, product interaction tracker. 2) Feature Store: User features (purchase history, preferences), Product features (categories, attributes), Real-time features (current session). 3) Model Training: Batch training pipeline (Spark), collaborative filtering + content-based hybrid, daily model updates to S3. 4) Serving Layer: Real-time inference service (low latency), model loaded in memory, feature enrichment at request time. 5) Cold Start Strategy: Content-based recommendations for new users (based on similar products viewed), popular items fallback, quick preference capture. 6) Real-time Updates: Kafka consumer updating user session features, immediate re-ranking based on browsing. 7) A/B Testing: Feature flag system for model variants, metrics collection per variant, statistical significance calculation, gradual rollout. Flow: Request -> Feature retrieval -> Model inference -> A/B variant selection -> Post-processing (dedup, inventory filter) -> Response. Monitoring: Recommendation click-through rate, conversion rate, diversity metrics.",
      "explanation": "Hybrid approach handles different scenarios. Real-time updates improve relevance. A/B testing enables safe model iteration.",
      "concept_ids": [
        "CONCEPT-056"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q141",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design service communication for an order processing system. Order Service needs to call Payment Service (critical, needs response) and Notification Service (non-critical, can fail). Which communication patterns?",
      "options": [
        "Synchronous call with Circuit Breaker for Payment, async message queue for Notification",
        "Synchronous calls for both with long timeouts",
        "Async message queue for both",
        "Direct database sharing"
      ],
      "correct_answer": "Synchronous call with Circuit Breaker for Payment, async message queue for Notification",
      "explanation": "Critical paths (Payment) need synchronous calls with resilience patterns. Non-critical paths (Notification) benefit from async decoupling - failure doesn't block order.",
      "concept_ids": [
        "CONCEPT-001",
        "CONCEPT-023"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q142",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a database migration strategy for moving from PostgreSQL monolith database to service-owned databases. Current state: 200 tables, 50 services reading/writing to shared database. Goal: Database per Service.",
      "correct_answer": "Migration Strategy: Phase 1 - Preparation: Catalog all tables and owning services, identify table dependencies, document cross-service queries. Phase 2 - API Layer: Create data access APIs in owning services before moving tables, redirect other services to use APIs instead of direct DB access. Phase 3 - Dual Write: Owning service writes to both old and new database, sync verified by reconciliation jobs. Phase 4 - Cutover: Switch reads to new database, old database becomes read-only, then decommission. Per-table approach: High-priority tables (critical/high-traffic) migrated first with more testing, Low-priority tables in batches. Cross-service queries: Replace with API calls or CQRS read models. Rollback plan: Maintain reverse sync capability until migration stable. Timeline: Expect 12-18 months for 200 tables. Tools: CDC for sync (Debezium), schema migration (Flyway), shadow traffic testing. Risk mitigation: Feature flags for routing, extensive testing, runbooks for issues.",
      "explanation": "Gradual migration with dual-write prevents data loss. API layer first decouples services before physical migration. Long timeline for safety.",
      "concept_ids": [
        "CONCEPT-005",
        "CONCEPT-008"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q143",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a structured error handling strategy for a public API. Errors need to be: actionable for clients, secure (no internal details exposed), consistent across services, and debuggable for operators.",
      "options": [
        "RFC 7807 Problem Details format with error codes, correlation IDs in response and logs, separate internal vs external error messages",
        "Stack traces in API responses",
        "Generic 500 Internal Server Error for all failures",
        "Error codes without descriptions"
      ],
      "correct_answer": "RFC 7807 Problem Details format with error codes, correlation IDs in response and logs, separate internal vs external error messages",
      "explanation": "RFC 7807 provides standard format. Error codes enable client handling. Correlation IDs link client errors to server logs. Separation protects internals.",
      "concept_ids": [
        "CONCEPT-066"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q144",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a distributed locking mechanism for a job scheduler that runs on multiple instances. Requirements: only one instance processes each job, lock must auto-expire if holder crashes, at-least-once processing with idempotency.",
      "correct_answer": "Distributed Lock Design: Lock store: Redis with SETNX or ZooKeeper/etcd for stronger guarantees. Lock acquisition: SETNX with expiry (fencing token pattern for safety). Lock structure: job_id as key, instance_id + timestamp + fencing_token as value. Auto-expiry: TTL on lock (e.g., 5 minutes), heartbeat extends TTL while processing. Fencing token: Monotonic token returned on lock acquisition, passed to downstream operations, storage rejects operations with stale tokens. Algorithm: 1) Try acquire lock with SETNX, 2) If acquired, start heartbeat thread, 3) Process job (pass fencing token), 4) Release lock on completion, 5) On crash, lock expires, job reprocessed by another instance. At-least-once: Failed jobs released for retry. Idempotency: Job processing must be idempotent (use job_id as idempotency key). Considerations: Clock skew for TTL, split-brain with network partition, Redlock for Redis multi-node safety. Monitoring: Lock acquisition latency, lock hold times, contested lock ratio.",
      "explanation": "Fencing tokens prevent split-brain issues. Auto-expiry handles crashes. Idempotency ensures correctness despite at-least-once processing.",
      "concept_ids": [
        "CONCEPT-047",
        "CONCEPT-024"
      ],
      "sources": [
        "Azure Patterns",
        "microservices.io"
      ]
    },
    {
      "id": "Q145",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a testing strategy for microservices. You have 15 services with complex dependencies. Full integration tests take 2 hours. How do you balance test coverage with developer productivity?",
      "options": [
        "Unit tests per service, contract tests for service boundaries, selective integration tests based on changes, full integration in nightly builds",
        "Only unit tests for speed",
        "Only full integration tests for coverage",
        "Manual testing only"
      ],
      "correct_answer": "Unit tests per service, contract tests for service boundaries, selective integration tests based on changes, full integration in nightly builds",
      "explanation": "Testing pyramid: fast unit tests for most coverage, contract tests catch interface breaks, selective integration for changed areas, comprehensive nightly for full validation.",
      "concept_ids": [
        "CONCEPT-083",
        "CONCEPT-052"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q146",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a GitOps-based deployment pipeline for 30 microservices. Requirements: environment promotion (dev -> staging -> prod), automated rollbacks, drift detection, secrets management integration.",
      "correct_answer": "GitOps Architecture: Repository structure: /apps (per-service configs), /base (shared configs), /environments (dev/staging/prod overlays using Kustomize). Tools: ArgoCD for sync, Kustomize for overlays, Sealed Secrets for secret management. Workflow: 1) Developer merges to main -> CI builds image -> updates dev overlay, 2) ArgoCD syncs dev automatically, 3) Promotion: PR from dev to staging overlay, 4) Manual approval for prod promotion. Environment configs: Base configs in /base, environment-specific in /environments/[env]/. Automated rollbacks: ArgoCD sync failure -> auto rollback to previous Git commit, alert to team. Drift detection: ArgoCD continuous sync with auto-heal, alerts on manual changes. Secrets: SealedSecrets or External Secrets Operator syncing from Vault. Monitoring: ArgoCD dashboard, sync status metrics, deployment frequency tracking. Rollback process: Revert commit in Git -> ArgoCD syncs automatically -> service restored. Branch strategy: Single main branch, environment promotion via directory overlays.",
      "explanation": "Git as single source of truth for all environments. Kustomize overlays handle environment differences. ArgoCD provides sync and rollback automation.",
      "concept_ids": [
        "CONCEPT-036",
        "CONCEPT-053"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q147",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a capacity planning approach for a rapidly growing startup. User base doubles every 6 months. Current infrastructure costs are manageable but you need to plan ahead. Which approach?",
      "options": [
        "Trend analysis with 3-6 month projections, auto-scaling configured aggressively, quarterly capacity reviews, load testing at 2x current peak",
        "Wait for performance issues",
        "Over-provision for 5 years of growth",
        "Manual scaling when customers complain"
      ],
      "correct_answer": "Trend analysis with 3-6 month projections, auto-scaling configured aggressively, quarterly capacity reviews, load testing at 2x current peak",
      "explanation": "Trend analysis anticipates needs. Auto-scaling handles variability. Regular reviews adjust plans. Load testing validates capacity before it's needed.",
      "concept_ids": [
        "CONCEPT-038",
        "CONCEPT-040"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q148",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a file upload and processing pipeline. Requirements: support files up to 5GB, virus scanning, format validation, thumbnail generation for images, metadata extraction, resumable uploads.",
      "correct_answer": "Architecture: 1) Upload Service: Receives multipart uploads, supports resumable uploads via pre-signed URLs for direct S3 upload with chunked transfer. 2) Pre-signed URL approach: Client requests upload URL, uploads directly to S3, S3 triggers processing on completion. 3) Processing Pipeline (Step Functions or queue-based): S3 trigger -> Virus Scan Lambda (ClamAV) -> Format Validation -> Route by type -> Image: Thumbnail Generator, Document: Metadata Extractor -> Update File Service with results. 4) Virus scanning: Dedicated scanning instances, quarantine bucket for suspicious files. 5) Resumable: S3 multipart upload with client-tracked parts. 6) Large files: Chunked upload to S3, parallel processing where possible. 7) Error handling: Failed processing -> DLQ -> alert -> manual review. 8) Status tracking: File Service maintains processing status, client polls or receives webhook. Storage: Original files in upload bucket, processed files in serving bucket, thumbnails in CDN-backed bucket. Security: Pre-signed URLs expire quickly, no direct bucket access.",
      "explanation": "Direct-to-S3 upload handles large files efficiently. Event-driven pipeline processes asynchronously. Each step is independent and scalable.",
      "concept_ids": [
        "CONCEPT-050",
        "CONCEPT-012"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q149",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design team structure for a microservices platform with 8 services. Each service should be owned by a team, but you only have 25 engineers. How should teams be organized?",
      "options": [
        "3-4 cross-functional teams, each owning 2-3 related services based on domain boundaries",
        "One team owning all services",
        "8 teams with 3 engineers each",
        "Separate teams for frontend, backend, and DevOps"
      ],
      "correct_answer": "3-4 cross-functional teams, each owning 2-3 related services based on domain boundaries",
      "explanation": "Two-pizza team size (6-8 people) owns related services within a domain. Cross-functional includes all needed skills. Aligns with Conway's Law.",
      "concept_ids": [
        "CONCEPT-043",
        "CONCEPT-016"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q150",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a multi-cluster Kubernetes strategy for a global application. Requirements: failover between clusters, centralized management, consistent policies, workload distribution based on regional user traffic.",
      "correct_answer": "Multi-Cluster Architecture: Cluster topology: Regional clusters (US, EU, APAC) + management cluster. Federation: Kubernetes Federation v2 or Rancher Fleet for multi-cluster management. Traffic routing: Global load balancer (CloudFlare/AWS Global Accelerator) routing based on latency. Cluster management: GitOps with Fleet/ArgoCD multi-cluster sync from single repo. Policy enforcement: OPA Gatekeeper with centralized policy repo, synced to all clusters. Workload distribution: Deployment manifests define regional targets, override for region-specific config. Failover: Health checks at global LB, automatic failover to next nearest healthy cluster, manual override capability. Service mesh: Istio multi-cluster for cross-cluster service discovery and traffic management. Observability: Centralized monitoring (Thanos for metrics federation), unified logging. DR: Velero backups per cluster to central location. Consistency: Base configs in central repo, regional overlays where needed.",
      "explanation": "Federation provides centralized management. GitOps ensures consistency. Global LB handles routing and failover. Centralized policies maintain governance.",
      "concept_ids": [
        "CONCEPT-046",
        "CONCEPT-036"
      ],
      "sources": [
        "Kubernetes KEPs"
      ]
    },
    {
      "id": "Q151",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design an incident response process for a 24/7 SaaS platform. Team of 20 engineers across 3 time zones. Need: clear escalation, minimal burnout, effective communication.",
      "options": [
        "Follow-the-sun on-call rotation, severity-based escalation matrix, incident commander role, post-incident reviews, runbooks for common issues",
        "Single person on-call 24/7",
        "No on-call, address issues next business day",
        "Entire team responds to every incident"
      ],
      "correct_answer": "Follow-the-sun on-call rotation, severity-based escalation matrix, incident commander role, post-incident reviews, runbooks for common issues",
      "explanation": "Follow-the-sun leverages time zones to avoid 24-hour on-call. Severity-based escalation prevents over-alerting. Incident commander coordinates response. Runbooks enable efficient resolution.",
      "concept_ids": [
        "CONCEPT-070",
        "CONCEPT-066"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q152",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a workflow orchestration system for complex, long-running business processes (e.g., loan application that takes weeks). Requirements: human tasks, conditional routing, SLA tracking, process versioning, audit trail.",
      "correct_answer": "Workflow Engine Design: Core components: Workflow Engine (orchestrator), Task Queue, Timer Service, Human Task Service, Audit Logger. Workflow definition: DSL or BPMN, stored as versioned documents. Execution: Each workflow instance has unique ID, state machine tracks current position. State persistence: Durable storage (PostgreSQL) for workflow state, event sourcing for history. Human tasks: Tasks pushed to assignee queues, UI for task claiming/completion, timeout handling, reassignment. Conditional routing: Expression evaluation engine for branching logic. SLA tracking: Timer Service schedules SLA checks, escalation events on breach. Versioning: Workflow definitions versioned, running instances continue on original version, new instances use latest, migration for long-running if needed. Audit: Every state transition logged with timestamp, actor, decision. Long-running: Workflow sleeps between activities, Timer Service wakes when needed. Integration: Activities call external services via adapters. Technology options: Temporal, Cadence, or Camunda for BPMN support. Monitoring: Dashboard for workflow status, SLA breach alerts, bottleneck identification.",
      "explanation": "Durable workflow engine handles long-running processes. Event sourcing provides audit trail. Versioning allows process evolution without disrupting running instances.",
      "concept_ids": [
        "CONCEPT-002",
        "CONCEPT-004"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q153",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design an integration strategy for a microservices platform that must connect with 20 different external partners. Each partner has different API formats, auth methods, and reliability characteristics.",
      "options": [
        "Integration Service with adapter per partner, circuit breakers per partner, async messaging for non-real-time, standardized internal events",
        "Direct calls from each microservice to partners",
        "Single universal API wrapper",
        "Manual integration scripts"
      ],
      "correct_answer": "Integration Service with adapter per partner, circuit breakers per partner, async messaging for non-real-time, standardized internal events",
      "explanation": "Adapter pattern handles API differences. Circuit breakers isolate partner failures. Async messaging decouples when possible. Internal events standardize consumption.",
      "concept_ids": [
        "CONCEPT-015",
        "CONCEPT-001"
      ],
      "sources": [
        "Azure Patterns"
      ]
    },
    {
      "id": "Q154",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a feature flag system for a microservices platform. Requirements: per-user targeting, percentage rollouts, kill switch capability, real-time updates, minimal latency impact.",
      "correct_answer": "Feature Flag System: Architecture: Flag configuration service (source of truth), SDKs in each service, edge caching for performance. Flag types: Boolean (on/off), Percentage rollout, User targeting (by ID, attributes), Environment-based. Configuration: Flag definitions in database/config store, admin UI for management, audit log for changes. SDK design: Initialize with config fetch, local cache with TTL, background refresh, fallback to defaults. Evaluation: SDK evaluates locally from cache (sub-ms), targeting rules evaluated in order: user overrides -> percentage hash -> default. Real-time updates: Webhook/SSE push to SDKs on flag change, or short polling (30s) as fallback. Kill switch: High-priority flag type, pushed immediately, SDKs prioritize kill switch updates. Caching: SDK maintains local cache, CDN for initial config fetch, Redis for shared state if needed. Monitoring: Flag evaluation metrics, percentage actual vs configured, latency of evaluation. Cleanup: Alert on old flags, archive unused, remove from code. Tools: LaunchDarkly, Split.io, or build on feature-flag-patterns.",
      "explanation": "Local SDK evaluation minimizes latency. Push updates enable real-time changes. Kill switch has fast path. Gradual rollout via consistent hashing on user ID.",
      "concept_ids": [
        "CONCEPT-056"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q155",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a compliance framework for a healthcare application handling PHI. Requirements: HIPAA compliance, audit logging, encryption, access controls, incident reporting.",
      "options": [
        "Encryption at rest and in transit, role-based access with audit logging, BAA with cloud provider, automated compliance scanning, incident response procedures",
        "Standard application security practices",
        "Encrypt only during transmission",
        "Annual security audit only"
      ],
      "correct_answer": "Encryption at rest and in transit, role-based access with audit logging, BAA with cloud provider, automated compliance scanning, incident response procedures",
      "explanation": "HIPAA requires comprehensive controls: encryption, access controls, auditing, business associate agreements, and documented incident response. Automation ensures ongoing compliance.",
      "concept_ids": [
        "CONCEPT-030",
        "CONCEPT-031"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q156",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a real-time bidding (RTB) system for an ad platform. Requirements: 100ms response time budget, 100K QPS, bid price prediction, budget pacing, winner notification.",
      "correct_answer": "RTB Architecture: Request flow: Ad request -> Load balancer -> Bid Service cluster. Bid Service: Stateless, horizontally scaled, in-memory feature cache. Latency budget: 20ms network, 30ms bid decision, 50ms margin. Components: 1) Feature Store: Pre-computed user/context features in Redis, updated async from data pipeline. 2) ML Model: Lightweight prediction model loaded in memory, predicts CTR/conversion probability. 3) Bid Calculator: Applies business rules, budget constraints, pacing to model output. 4) Budget Tracker: Real-time spend tracking in Redis (atomic increments), pacing algorithm limits bid rate. 5) Response Builder: Formats bid response, signs for verification. Scaling for 100K QPS: 50 bid service instances (2K QPS each), Redis cluster for features, horizontal scaling for peaks. Winner notification: Async webhook receiver, updates conversion tracking. Monitoring: Latency percentiles, win rate, spend rate, budget pacing accuracy. Optimization: Minimal allocations in hot path, connection pooling, pre-warmed feature cache. Fallback: No-bid response on timeout or error.",
      "explanation": "Extreme latency constraint requires in-memory processing. Pre-computed features avoid runtime lookups. Horizontal scaling handles high QPS.",
      "concept_ids": [
        "CONCEPT-032",
        "CONCEPT-040"
      ],
      "sources": [
        "microservices.io"
      ]
    },
    {
      "id": "Q157",
      "skill": "design",
      "topic": "decomposition",
      "difficulty": "medium",
      "format": "mcq",
      "question": "You're designing a platform that will be white-labeled for multiple enterprise customers. Each customer wants customization: branding, feature toggles, custom fields. What's the best multi-tenancy approach?",
      "options": [
        "Shared infrastructure with tenant-aware customization layer, configuration per tenant, feature flags for optional features",
        "Separate deployment per customer",
        "Fork the codebase per customer",
        "No customization allowed"
      ],
      "correct_answer": "Shared infrastructure with tenant-aware customization layer, configuration per tenant, feature flags for optional features",
      "explanation": "Shared infrastructure reduces operational burden. Tenant-aware customization layer handles branding/fields. Feature flags enable per-tenant features. Single codebase is maintainable.",
      "concept_ids": [
        "CONCEPT-056",
        "CONCEPT-081"
      ],
      "sources": [
        "ADRs"
      ]
    },
    {
      "id": "Q158",
      "skill": "design",
      "topic": "cloud_deployment",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a data replication strategy for a globally distributed database. Requirements: sub-50ms local reads, eventual consistency for non-critical data, strong consistency for financial data, conflict-free collaboration features.",
      "correct_answer": "Replication Strategy: Topology: Multi-master in 3 regions (US, EU, APAC). Data classification: 1) Financial data: Synchronous replication, single primary per record (assigned by region), cross-region writes route to primary. 2) User preferences: Async replication, LWW (last-writer-wins) conflict resolution. 3) Collaboration data: CRDTs for conflict-free merging (counters, sets, text). Local reads: All data replicated to local region, reads served locally (sub-50ms). Write path: Financial -> route to primary -> sync replicate -> ack. Preference -> local write -> async replicate. Collaboration -> local CRDT merge -> async sync. Conflict resolution: Financial: no conflicts (single writer), Preferences: timestamp-based LWW, Collaboration: CRDT rules (automatic merge). Lag monitoring: Replication lag alerts, fallback to primary read if lag exceeds threshold. Technology: CockroachDB for SQL with consistency options, or custom with Cassandra + CRDT layer. Consistency tuning: Per-query consistency level, not per-table.",
      "explanation": "Different consistency needs require different strategies. CRDTs solve collaboration conflicts. Routing to primary prevents financial data conflicts.",
      "concept_ids": [
        "CONCEPT-042",
        "CONCEPT-041"
      ],
      "sources": [
        "Martin Fowler"
      ]
    },
    {
      "id": "Q159",
      "skill": "design",
      "topic": "quality_attributes",
      "difficulty": "medium",
      "format": "mcq",
      "question": "Design a sustainable architecture for a cloud application. The company has committed to carbon neutrality. Which design choices most directly reduce environmental impact?",
      "options": [
        "Right-size resources for high utilization, schedule non-critical jobs for low-carbon periods, use serverless for variable workloads, prefer managed services",
        "Use the largest instances available",
        "Run 24/7 at peak capacity",
        "Deploy in multiple regions for redundancy only"
      ],
      "correct_answer": "Right-size resources for high utilization, schedule non-critical jobs for low-carbon periods, use serverless for variable workloads, prefer managed services",
      "explanation": "Higher utilization means less idle capacity. Scheduling for low-carbon grid periods reduces emissions. Serverless and managed services share resources efficiently.",
      "concept_ids": [
        "CONCEPT-071"
      ],
      "sources": [
        "AWS Well-Architected"
      ]
    },
    {
      "id": "Q160",
      "skill": "design",
      "topic": "architectural_patterns",
      "difficulty": "hard",
      "format": "free_response",
      "question": "Design a GraphQL federation for a microservices architecture with 10 backend services. Requirements: unified schema for clients, service autonomy for teams, performance optimization, caching strategy.",
      "correct_answer": "GraphQL Federation Design: Architecture: Apollo Federation with gateway. Each service: Subgraph defining its portion of schema with @key directives for entity references. Gateway: Apollo Router or Gateway, composes subgraphs at startup, routes queries to appropriate subgraphs. Schema stitching: Entities define @key fields, other services use @extends to add fields to entities. Service autonomy: Each team owns their subgraph schema, deploy independently, gateway recomposes on restart. Performance: Query planning at gateway (minimize round trips), DataLoader pattern for N+1 prevention, batched subgraph calls. Caching: Response caching at gateway (Cache-Control hints from subgraphs), CDN for public queries, Redis for authenticated user caching, persisted queries for frequent operations. Monitoring: Per-subgraph latency, resolver timing, error rates, query complexity metrics. Tooling: Schema registry for version management, breaking change detection, schema linting. Tracing: Distributed tracing through gateway to subgraphs.",
      "explanation": "Federation enables service autonomy while presenting unified schema. Gateway handles composition and optimization. DataLoader prevents N+1 queries.",
      "concept_ids": [
        "CONCEPT-006",
        "CONCEPT-065"
      ],
      "sources": [
        "microservices.io"
      ]
    }
  ]
}